{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [],
   "source": [
    "# essentials\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "import torch\n",
    "\n",
    "# loading data in\n",
    "from torchvision.datasets import ImageFolder\n",
    "from torchvision import transforms as T\n",
    "\n",
    "# putting data into batches (prevent memory overload)\n",
    "from torch.utils.data import DataLoader\n",
    "\n",
    "# for creating model\n",
    "from torch import nn,optim"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 67,
   "metadata": {},
   "outputs": [],
   "source": [
    "# we augment the training data to reduce overfitting on validation set and \n",
    "# eventual use on the web app\n",
    "# we randomly flip/rotate the training image\n",
    "\n",
    "# we convert images to tensor objects in both training and validation\n",
    "\n",
    "train_augs = T.Compose([\n",
    "    T.Grayscale(1),\n",
    "    T.RandomHorizontalFlip(p = 0.5),\n",
    "    T.RandomRotation(degrees=(-15, + 15)),\n",
    "    T.ToTensor()\n",
    "])\n",
    "\n",
    "valid_augs = T.Compose([\n",
    "    T.Grayscale(1),\n",
    "    T.ToTensor()\n",
    "])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 68,
   "metadata": {},
   "outputs": [],
   "source": [
    "# loading the data in using the helpful torchvision ImageFolder lib\n",
    "# (this lets us skip manually creation of the dataset)\n",
    "# we apply the transforms here\n",
    "\n",
    "train_path = \"C:\\\\Users\\\\fuchsga\\\\Desktop\\\\WebApps\\\\FacialAnalysisWebApp\\\\images\\\\train\"\n",
    "vildation_path = \"C:\\\\Users\\\\fuchsga\\\\Desktop\\\\WebApps\\\\FacialAnalysisWebApp\\\\images\\\\validation\"\n",
    "\n",
    "trainset = ImageFolder(train_path, transform = train_augs)\n",
    "validset = ImageFolder(vildation_path, transform = valid_augs)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 69,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'angry': 0, 'disgust': 1, 'fear': 2, 'happy': 3, 'neutral': 4, 'sad': 5, 'surprise': 6}\n"
     ]
    }
   ],
   "source": [
    "print(trainset.class_to_idx)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 137,
   "metadata": {},
   "outputs": [],
   "source": [
    "batch_size = 64\n",
    "\n",
    "train_loader = DataLoader(trainset, batch_size = batch_size, shuffle = True)\n",
    "valid_loader = DataLoader(validset, batch_size = batch_size)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 263,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "M5(\n",
      "  (layer1): Sequential(\n",
      "    (0): Conv2d(1, 32, kernel_size=(5, 5), stride=(1, 1))\n",
      "    (1): ReLU()\n",
      "    (2): MaxPool2d(kernel_size=2, stride=2, padding=0, dilation=1, ceil_mode=False)\n",
      "    (3): Dropout(p=0.2, inplace=False)\n",
      "  )\n",
      "  (layer2): Sequential(\n",
      "    (0): Conv2d(32, 64, kernel_size=(3, 3), stride=(1, 1))\n",
      "    (1): ReLU()\n",
      "    (2): MaxPool2d(kernel_size=2, stride=2, padding=0, dilation=1, ceil_mode=False)\n",
      "    (3): Dropout(p=0.2, inplace=False)\n",
      "  )\n",
      "  (layer3): Sequential(\n",
      "    (0): Conv2d(64, 64, kernel_size=(3, 3), stride=(1, 1))\n",
      "    (1): ReLU()\n",
      "    (2): MaxPool2d(kernel_size=2, stride=2, padding=0, dilation=1, ceil_mode=False)\n",
      "    (3): Dropout(p=0.2, inplace=False)\n",
      "  )\n",
      "  (layer4): Sequential(\n",
      "    (0): Flatten(start_dim=1, end_dim=-1)\n",
      "    (1): Linear(in_features=1024, out_features=128, bias=True)\n",
      "  )\n",
      "  (layer5): Sequential(\n",
      "    (0): Flatten(start_dim=1, end_dim=-1)\n",
      "    (1): Linear(in_features=128, out_features=7, bias=True)\n",
      "  )\n",
      ")\n",
      "Number of parameters: 188359\n"
     ]
    }
   ],
   "source": [
    "class M5(nn.Module):\n",
    "    def __init__(self, n_input=1, n_output=7, n_channel=32, drop=0.2):\n",
    "        super().__init__()\n",
    "\n",
    "        self.layer1 = nn.Sequential(\n",
    "            nn.Conv2d(n_input, n_channel, kernel_size=5),\n",
    "            nn.ReLU(),\n",
    "            nn.MaxPool2d(2),\n",
    "            nn.Dropout(drop)\n",
    "            )\n",
    "        self.layer2 = nn.Sequential(\n",
    "            nn.Conv2d(n_channel, 2*n_channel, kernel_size=3),\n",
    "            nn.ReLU(),\n",
    "            nn.MaxPool2d(2),\n",
    "            nn.Dropout(drop)\n",
    "            )\n",
    "        self.layer3 = nn.Sequential(\n",
    "            nn.Conv2d(2*n_channel, 2*n_channel, kernel_size=3),\n",
    "            nn.ReLU(),\n",
    "            nn.MaxPool2d(2),\n",
    "            nn.Dropout(drop)\n",
    "            )\n",
    "        self.layer4 = nn.Sequential(\n",
    "            nn.Flatten(),\n",
    "            nn.Linear(1024, 128)\n",
    "        )\n",
    "        self.layer5 = nn.Sequential(\n",
    "            nn.Flatten(),\n",
    "            nn.Linear(128, n_output)\n",
    "        )\n",
    "\n",
    "    def forward(self, x):\n",
    "        x = self.layer1(x)\n",
    "        x = self.layer2(x)\n",
    "        x = self.layer3(x)\n",
    "        x = self.layer4(x)\n",
    "        x = self.layer5(x)\n",
    "        return nn.functional.log_softmax(x)\n",
    "\n",
    "device = 'cuda'\n",
    "\n",
    "model = M5(n_output=len(trainset.classes))\n",
    "model.to(device)\n",
    "print(model)\n",
    "\n",
    "n = sum(p.numel() for p in model.parameters())\n",
    "print(\"Number of parameters: %s\" % n)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 264,
   "metadata": {},
   "outputs": [],
   "source": [
    "optimizer = optim.Adam(model.parameters(), lr=0.001, weight_decay=0.0001)\n",
    "scheduler = optim.lr_scheduler.StepLR(optimizer, step_size=20, gamma=0.1)  # reduce the learning after 20 epochs by a factor of 10"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 265,
   "metadata": {},
   "outputs": [],
   "source": [
    "def train(model, epoch, log_interval):\n",
    "    model.train()\n",
    "    for batch_idx, (data, target) in enumerate(train_loader):\n",
    "\n",
    "        data = data.to(device)\n",
    "        target = target.to(device)\n",
    "\n",
    "        output = model(data)\n",
    "\n",
    "        # negative log-likelihood for a tensor of size (batch x 1 x n_output)\n",
    "        loss = nn.functional.nll_loss(output.squeeze(), target)\n",
    "\n",
    "        optimizer.zero_grad()\n",
    "        loss.backward()\n",
    "        optimizer.step()\n",
    "\n",
    "        # print training stats\n",
    "        if batch_idx % log_interval == 0:\n",
    "            print(f\"Train Epoch: {epoch} [{batch_idx * len(data)}/{len(train_loader.dataset)} ({100. * batch_idx / len(train_loader):.0f}%)]\\tLoss: {loss.item():.6f}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 266,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\fuchsga\\AppData\\Local\\Temp\\ipykernel_29756\\2981391630.py:38: UserWarning: Implicit dimension choice for log_softmax has been deprecated. Change the call to include dim=X as an argument.\n",
      "  return nn.functional.log_softmax(x)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train Epoch: 1 [0/28821 (0%)]\tLoss: 1.971704\n",
      "Train Epoch: 1 [1280/28821 (4%)]\tLoss: 1.880917\n",
      "Train Epoch: 1 [2560/28821 (9%)]\tLoss: 1.883081\n",
      "Train Epoch: 1 [3840/28821 (13%)]\tLoss: 1.763613\n",
      "Train Epoch: 1 [5120/28821 (18%)]\tLoss: 1.688683\n",
      "Train Epoch: 1 [6400/28821 (22%)]\tLoss: 1.972862\n",
      "Train Epoch: 1 [7680/28821 (27%)]\tLoss: 1.703659\n",
      "Train Epoch: 1 [8960/28821 (31%)]\tLoss: 1.661169\n",
      "Train Epoch: 1 [10240/28821 (35%)]\tLoss: 1.839781\n",
      "Train Epoch: 1 [11520/28821 (40%)]\tLoss: 1.886461\n",
      "Train Epoch: 1 [12800/28821 (44%)]\tLoss: 1.755640\n",
      "Train Epoch: 1 [14080/28821 (49%)]\tLoss: 1.663437\n",
      "Train Epoch: 1 [15360/28821 (53%)]\tLoss: 1.668988\n",
      "Train Epoch: 1 [16640/28821 (58%)]\tLoss: 1.676291\n",
      "Train Epoch: 1 [17920/28821 (62%)]\tLoss: 1.770128\n",
      "Train Epoch: 1 [19200/28821 (67%)]\tLoss: 1.601039\n",
      "Train Epoch: 1 [20480/28821 (71%)]\tLoss: 1.741401\n",
      "Train Epoch: 1 [21760/28821 (75%)]\tLoss: 1.732704\n",
      "Train Epoch: 1 [23040/28821 (80%)]\tLoss: 1.768159\n",
      "Train Epoch: 1 [24320/28821 (84%)]\tLoss: 1.626320\n",
      "Train Epoch: 1 [25600/28821 (89%)]\tLoss: 1.624604\n",
      "Train Epoch: 1 [26880/28821 (93%)]\tLoss: 1.648153\n",
      "Train Epoch: 1 [28160/28821 (98%)]\tLoss: 1.734319\n",
      "Train Epoch: 2 [0/28821 (0%)]\tLoss: 1.617604\n",
      "Train Epoch: 2 [1280/28821 (4%)]\tLoss: 1.499510\n",
      "Train Epoch: 2 [2560/28821 (9%)]\tLoss: 1.701364\n",
      "Train Epoch: 2 [3840/28821 (13%)]\tLoss: 1.515702\n",
      "Train Epoch: 2 [5120/28821 (18%)]\tLoss: 1.603889\n",
      "Train Epoch: 2 [6400/28821 (22%)]\tLoss: 1.554101\n",
      "Train Epoch: 2 [7680/28821 (27%)]\tLoss: 1.657010\n",
      "Train Epoch: 2 [8960/28821 (31%)]\tLoss: 1.509544\n",
      "Train Epoch: 2 [10240/28821 (35%)]\tLoss: 1.526797\n",
      "Train Epoch: 2 [11520/28821 (40%)]\tLoss: 1.367578\n",
      "Train Epoch: 2 [12800/28821 (44%)]\tLoss: 1.641535\n",
      "Train Epoch: 2 [14080/28821 (49%)]\tLoss: 1.707216\n",
      "Train Epoch: 2 [15360/28821 (53%)]\tLoss: 1.512947\n",
      "Train Epoch: 2 [16640/28821 (58%)]\tLoss: 1.428934\n",
      "Train Epoch: 2 [17920/28821 (62%)]\tLoss: 1.664168\n",
      "Train Epoch: 2 [19200/28821 (67%)]\tLoss: 1.619682\n",
      "Train Epoch: 2 [20480/28821 (71%)]\tLoss: 1.349233\n",
      "Train Epoch: 2 [21760/28821 (75%)]\tLoss: 1.497802\n",
      "Train Epoch: 2 [23040/28821 (80%)]\tLoss: 1.462452\n",
      "Train Epoch: 2 [24320/28821 (84%)]\tLoss: 1.578880\n",
      "Train Epoch: 2 [25600/28821 (89%)]\tLoss: 1.644999\n",
      "Train Epoch: 2 [26880/28821 (93%)]\tLoss: 1.464742\n",
      "Train Epoch: 2 [28160/28821 (98%)]\tLoss: 1.566830\n",
      "Train Epoch: 3 [0/28821 (0%)]\tLoss: 1.258158\n",
      "Train Epoch: 3 [1280/28821 (4%)]\tLoss: 1.269006\n",
      "Train Epoch: 3 [2560/28821 (9%)]\tLoss: 1.316290\n",
      "Train Epoch: 3 [3840/28821 (13%)]\tLoss: 1.437139\n",
      "Train Epoch: 3 [5120/28821 (18%)]\tLoss: 1.384385\n",
      "Train Epoch: 3 [6400/28821 (22%)]\tLoss: 1.218222\n",
      "Train Epoch: 3 [7680/28821 (27%)]\tLoss: 1.395548\n",
      "Train Epoch: 3 [8960/28821 (31%)]\tLoss: 1.431142\n",
      "Train Epoch: 3 [10240/28821 (35%)]\tLoss: 1.496519\n",
      "Train Epoch: 3 [11520/28821 (40%)]\tLoss: 1.474352\n",
      "Train Epoch: 3 [12800/28821 (44%)]\tLoss: 1.448665\n",
      "Train Epoch: 3 [14080/28821 (49%)]\tLoss: 1.649387\n",
      "Train Epoch: 3 [15360/28821 (53%)]\tLoss: 1.422360\n",
      "Train Epoch: 3 [16640/28821 (58%)]\tLoss: 1.615665\n",
      "Train Epoch: 3 [17920/28821 (62%)]\tLoss: 1.326105\n",
      "Train Epoch: 3 [19200/28821 (67%)]\tLoss: 1.656889\n",
      "Train Epoch: 3 [20480/28821 (71%)]\tLoss: 1.354661\n",
      "Train Epoch: 3 [21760/28821 (75%)]\tLoss: 1.492968\n",
      "Train Epoch: 3 [23040/28821 (80%)]\tLoss: 1.500678\n",
      "Train Epoch: 3 [24320/28821 (84%)]\tLoss: 1.408338\n",
      "Train Epoch: 3 [25600/28821 (89%)]\tLoss: 1.660050\n",
      "Train Epoch: 3 [26880/28821 (93%)]\tLoss: 1.263259\n",
      "Train Epoch: 3 [28160/28821 (98%)]\tLoss: 1.544548\n",
      "Train Epoch: 4 [0/28821 (0%)]\tLoss: 1.321146\n",
      "Train Epoch: 4 [1280/28821 (4%)]\tLoss: 1.349461\n",
      "Train Epoch: 4 [2560/28821 (9%)]\tLoss: 1.603692\n",
      "Train Epoch: 4 [3840/28821 (13%)]\tLoss: 1.443696\n",
      "Train Epoch: 4 [5120/28821 (18%)]\tLoss: 1.404928\n",
      "Train Epoch: 4 [6400/28821 (22%)]\tLoss: 1.309423\n",
      "Train Epoch: 4 [7680/28821 (27%)]\tLoss: 1.626438\n",
      "Train Epoch: 4 [8960/28821 (31%)]\tLoss: 1.549942\n",
      "Train Epoch: 4 [10240/28821 (35%)]\tLoss: 1.245280\n",
      "Train Epoch: 4 [11520/28821 (40%)]\tLoss: 1.319551\n",
      "Train Epoch: 4 [12800/28821 (44%)]\tLoss: 1.484913\n",
      "Train Epoch: 4 [14080/28821 (49%)]\tLoss: 1.392097\n",
      "Train Epoch: 4 [15360/28821 (53%)]\tLoss: 1.398318\n",
      "Train Epoch: 4 [16640/28821 (58%)]\tLoss: 1.519878\n",
      "Train Epoch: 4 [17920/28821 (62%)]\tLoss: 1.458953\n",
      "Train Epoch: 4 [19200/28821 (67%)]\tLoss: 1.419822\n",
      "Train Epoch: 4 [20480/28821 (71%)]\tLoss: 1.249832\n",
      "Train Epoch: 4 [21760/28821 (75%)]\tLoss: 1.318285\n",
      "Train Epoch: 4 [23040/28821 (80%)]\tLoss: 1.346281\n",
      "Train Epoch: 4 [24320/28821 (84%)]\tLoss: 1.334185\n",
      "Train Epoch: 4 [25600/28821 (89%)]\tLoss: 1.433207\n",
      "Train Epoch: 4 [26880/28821 (93%)]\tLoss: 1.337418\n",
      "Train Epoch: 4 [28160/28821 (98%)]\tLoss: 1.549183\n",
      "Train Epoch: 5 [0/28821 (0%)]\tLoss: 1.499495\n",
      "Train Epoch: 5 [1280/28821 (4%)]\tLoss: 1.456078\n",
      "Train Epoch: 5 [2560/28821 (9%)]\tLoss: 1.499480\n",
      "Train Epoch: 5 [3840/28821 (13%)]\tLoss: 1.336225\n",
      "Train Epoch: 5 [5120/28821 (18%)]\tLoss: 1.375980\n",
      "Train Epoch: 5 [6400/28821 (22%)]\tLoss: 1.507709\n",
      "Train Epoch: 5 [7680/28821 (27%)]\tLoss: 1.402230\n",
      "Train Epoch: 5 [8960/28821 (31%)]\tLoss: 1.026032\n",
      "Train Epoch: 5 [10240/28821 (35%)]\tLoss: 1.514489\n",
      "Train Epoch: 5 [11520/28821 (40%)]\tLoss: 1.431810\n",
      "Train Epoch: 5 [12800/28821 (44%)]\tLoss: 1.300999\n",
      "Train Epoch: 5 [14080/28821 (49%)]\tLoss: 1.510940\n",
      "Train Epoch: 5 [15360/28821 (53%)]\tLoss: 1.633288\n",
      "Train Epoch: 5 [16640/28821 (58%)]\tLoss: 1.480287\n",
      "Train Epoch: 5 [17920/28821 (62%)]\tLoss: 1.525942\n",
      "Train Epoch: 5 [19200/28821 (67%)]\tLoss: 1.194400\n",
      "Train Epoch: 5 [20480/28821 (71%)]\tLoss: 1.391704\n",
      "Train Epoch: 5 [21760/28821 (75%)]\tLoss: 1.354965\n",
      "Train Epoch: 5 [23040/28821 (80%)]\tLoss: 1.511702\n",
      "Train Epoch: 5 [24320/28821 (84%)]\tLoss: 1.373947\n",
      "Train Epoch: 5 [25600/28821 (89%)]\tLoss: 1.425893\n",
      "Train Epoch: 5 [26880/28821 (93%)]\tLoss: 1.284807\n",
      "Train Epoch: 5 [28160/28821 (98%)]\tLoss: 1.263266\n",
      "Train Epoch: 6 [0/28821 (0%)]\tLoss: 1.311896\n",
      "Train Epoch: 6 [1280/28821 (4%)]\tLoss: 1.118508\n",
      "Train Epoch: 6 [2560/28821 (9%)]\tLoss: 1.254080\n",
      "Train Epoch: 6 [3840/28821 (13%)]\tLoss: 1.300465\n",
      "Train Epoch: 6 [5120/28821 (18%)]\tLoss: 1.477845\n",
      "Train Epoch: 6 [6400/28821 (22%)]\tLoss: 1.079947\n",
      "Train Epoch: 6 [7680/28821 (27%)]\tLoss: 1.385324\n",
      "Train Epoch: 6 [8960/28821 (31%)]\tLoss: 1.509714\n",
      "Train Epoch: 6 [10240/28821 (35%)]\tLoss: 1.304782\n",
      "Train Epoch: 6 [11520/28821 (40%)]\tLoss: 1.506071\n",
      "Train Epoch: 6 [12800/28821 (44%)]\tLoss: 1.252701\n",
      "Train Epoch: 6 [14080/28821 (49%)]\tLoss: 1.610430\n",
      "Train Epoch: 6 [15360/28821 (53%)]\tLoss: 1.258543\n",
      "Train Epoch: 6 [16640/28821 (58%)]\tLoss: 1.539199\n",
      "Train Epoch: 6 [17920/28821 (62%)]\tLoss: 1.428632\n",
      "Train Epoch: 6 [19200/28821 (67%)]\tLoss: 1.286710\n",
      "Train Epoch: 6 [20480/28821 (71%)]\tLoss: 1.110034\n",
      "Train Epoch: 6 [21760/28821 (75%)]\tLoss: 1.538682\n",
      "Train Epoch: 6 [23040/28821 (80%)]\tLoss: 1.537782\n",
      "Train Epoch: 6 [24320/28821 (84%)]\tLoss: 1.709689\n",
      "Train Epoch: 6 [25600/28821 (89%)]\tLoss: 1.263005\n",
      "Train Epoch: 6 [26880/28821 (93%)]\tLoss: 1.455416\n",
      "Train Epoch: 6 [28160/28821 (98%)]\tLoss: 1.211522\n",
      "Train Epoch: 7 [0/28821 (0%)]\tLoss: 1.515503\n",
      "Train Epoch: 7 [1280/28821 (4%)]\tLoss: 1.218464\n",
      "Train Epoch: 7 [2560/28821 (9%)]\tLoss: 1.259201\n",
      "Train Epoch: 7 [3840/28821 (13%)]\tLoss: 1.110327\n",
      "Train Epoch: 7 [5120/28821 (18%)]\tLoss: 1.341767\n",
      "Train Epoch: 7 [6400/28821 (22%)]\tLoss: 1.348014\n",
      "Train Epoch: 7 [7680/28821 (27%)]\tLoss: 1.384931\n",
      "Train Epoch: 7 [8960/28821 (31%)]\tLoss: 1.571966\n",
      "Train Epoch: 7 [10240/28821 (35%)]\tLoss: 1.293789\n",
      "Train Epoch: 7 [11520/28821 (40%)]\tLoss: 1.460189\n",
      "Train Epoch: 7 [12800/28821 (44%)]\tLoss: 1.362740\n",
      "Train Epoch: 7 [14080/28821 (49%)]\tLoss: 1.272091\n",
      "Train Epoch: 7 [15360/28821 (53%)]\tLoss: 1.301618\n",
      "Train Epoch: 7 [16640/28821 (58%)]\tLoss: 1.291269\n",
      "Train Epoch: 7 [17920/28821 (62%)]\tLoss: 1.442463\n",
      "Train Epoch: 7 [19200/28821 (67%)]\tLoss: 1.348485\n",
      "Train Epoch: 7 [20480/28821 (71%)]\tLoss: 1.406343\n",
      "Train Epoch: 7 [21760/28821 (75%)]\tLoss: 1.220957\n",
      "Train Epoch: 7 [23040/28821 (80%)]\tLoss: 1.255949\n",
      "Train Epoch: 7 [24320/28821 (84%)]\tLoss: 1.343675\n",
      "Train Epoch: 7 [25600/28821 (89%)]\tLoss: 1.290736\n",
      "Train Epoch: 7 [26880/28821 (93%)]\tLoss: 1.300248\n",
      "Train Epoch: 7 [28160/28821 (98%)]\tLoss: 1.237512\n",
      "Train Epoch: 8 [0/28821 (0%)]\tLoss: 1.311254\n",
      "Train Epoch: 8 [1280/28821 (4%)]\tLoss: 1.340669\n",
      "Train Epoch: 8 [2560/28821 (9%)]\tLoss: 1.280822\n",
      "Train Epoch: 8 [3840/28821 (13%)]\tLoss: 1.324870\n",
      "Train Epoch: 8 [5120/28821 (18%)]\tLoss: 1.403132\n",
      "Train Epoch: 8 [6400/28821 (22%)]\tLoss: 1.273765\n",
      "Train Epoch: 8 [7680/28821 (27%)]\tLoss: 1.253499\n",
      "Train Epoch: 8 [8960/28821 (31%)]\tLoss: 1.238492\n",
      "Train Epoch: 8 [10240/28821 (35%)]\tLoss: 1.252027\n",
      "Train Epoch: 8 [11520/28821 (40%)]\tLoss: 1.237515\n",
      "Train Epoch: 8 [12800/28821 (44%)]\tLoss: 1.323324\n",
      "Train Epoch: 8 [14080/28821 (49%)]\tLoss: 1.261708\n",
      "Train Epoch: 8 [15360/28821 (53%)]\tLoss: 1.269222\n",
      "Train Epoch: 8 [16640/28821 (58%)]\tLoss: 1.241216\n",
      "Train Epoch: 8 [17920/28821 (62%)]\tLoss: 1.441032\n",
      "Train Epoch: 8 [19200/28821 (67%)]\tLoss: 1.250019\n",
      "Train Epoch: 8 [20480/28821 (71%)]\tLoss: 1.125335\n",
      "Train Epoch: 8 [21760/28821 (75%)]\tLoss: 1.495848\n",
      "Train Epoch: 8 [23040/28821 (80%)]\tLoss: 1.406510\n",
      "Train Epoch: 8 [24320/28821 (84%)]\tLoss: 1.404911\n",
      "Train Epoch: 8 [25600/28821 (89%)]\tLoss: 1.277097\n",
      "Train Epoch: 8 [26880/28821 (93%)]\tLoss: 1.275732\n",
      "Train Epoch: 8 [28160/28821 (98%)]\tLoss: 1.312693\n",
      "Train Epoch: 9 [0/28821 (0%)]\tLoss: 1.416960\n",
      "Train Epoch: 9 [1280/28821 (4%)]\tLoss: 1.615955\n",
      "Train Epoch: 9 [2560/28821 (9%)]\tLoss: 1.284666\n",
      "Train Epoch: 9 [3840/28821 (13%)]\tLoss: 1.325008\n",
      "Train Epoch: 9 [5120/28821 (18%)]\tLoss: 1.305562\n",
      "Train Epoch: 9 [6400/28821 (22%)]\tLoss: 1.400895\n",
      "Train Epoch: 9 [7680/28821 (27%)]\tLoss: 1.265166\n",
      "Train Epoch: 9 [8960/28821 (31%)]\tLoss: 1.352093\n",
      "Train Epoch: 9 [10240/28821 (35%)]\tLoss: 1.199346\n",
      "Train Epoch: 9 [11520/28821 (40%)]\tLoss: 1.437900\n",
      "Train Epoch: 9 [12800/28821 (44%)]\tLoss: 1.201789\n",
      "Train Epoch: 9 [14080/28821 (49%)]\tLoss: 1.260101\n",
      "Train Epoch: 9 [15360/28821 (53%)]\tLoss: 1.514945\n",
      "Train Epoch: 9 [16640/28821 (58%)]\tLoss: 1.268504\n",
      "Train Epoch: 9 [17920/28821 (62%)]\tLoss: 1.356444\n",
      "Train Epoch: 9 [19200/28821 (67%)]\tLoss: 1.311457\n",
      "Train Epoch: 9 [20480/28821 (71%)]\tLoss: 1.089026\n",
      "Train Epoch: 9 [21760/28821 (75%)]\tLoss: 1.309707\n",
      "Train Epoch: 9 [23040/28821 (80%)]\tLoss: 1.236684\n",
      "Train Epoch: 9 [24320/28821 (84%)]\tLoss: 1.227614\n",
      "Train Epoch: 9 [25600/28821 (89%)]\tLoss: 1.218476\n",
      "Train Epoch: 9 [26880/28821 (93%)]\tLoss: 1.449182\n",
      "Train Epoch: 9 [28160/28821 (98%)]\tLoss: 1.379558\n",
      "Train Epoch: 10 [0/28821 (0%)]\tLoss: 1.201315\n",
      "Train Epoch: 10 [1280/28821 (4%)]\tLoss: 1.231968\n",
      "Train Epoch: 10 [2560/28821 (9%)]\tLoss: 1.184769\n",
      "Train Epoch: 10 [3840/28821 (13%)]\tLoss: 1.223687\n",
      "Train Epoch: 10 [5120/28821 (18%)]\tLoss: 1.250027\n",
      "Train Epoch: 10 [6400/28821 (22%)]\tLoss: 1.175197\n",
      "Train Epoch: 10 [7680/28821 (27%)]\tLoss: 1.208768\n",
      "Train Epoch: 10 [8960/28821 (31%)]\tLoss: 1.324807\n",
      "Train Epoch: 10 [10240/28821 (35%)]\tLoss: 1.393234\n",
      "Train Epoch: 10 [11520/28821 (40%)]\tLoss: 1.292444\n",
      "Train Epoch: 10 [12800/28821 (44%)]\tLoss: 1.368825\n",
      "Train Epoch: 10 [14080/28821 (49%)]\tLoss: 1.212145\n",
      "Train Epoch: 10 [15360/28821 (53%)]\tLoss: 1.167501\n",
      "Train Epoch: 10 [16640/28821 (58%)]\tLoss: 1.258833\n",
      "Train Epoch: 10 [17920/28821 (62%)]\tLoss: 1.275478\n",
      "Train Epoch: 10 [19200/28821 (67%)]\tLoss: 1.304259\n",
      "Train Epoch: 10 [20480/28821 (71%)]\tLoss: 1.496818\n",
      "Train Epoch: 10 [21760/28821 (75%)]\tLoss: 1.323839\n",
      "Train Epoch: 10 [23040/28821 (80%)]\tLoss: 1.336658\n",
      "Train Epoch: 10 [24320/28821 (84%)]\tLoss: 1.158604\n",
      "Train Epoch: 10 [25600/28821 (89%)]\tLoss: 1.399575\n",
      "Train Epoch: 10 [26880/28821 (93%)]\tLoss: 1.430210\n",
      "Train Epoch: 10 [28160/28821 (98%)]\tLoss: 1.422931\n",
      "Train Epoch: 11 [0/28821 (0%)]\tLoss: 1.091745\n",
      "Train Epoch: 11 [1280/28821 (4%)]\tLoss: 1.536955\n",
      "Train Epoch: 11 [2560/28821 (9%)]\tLoss: 1.303079\n",
      "Train Epoch: 11 [3840/28821 (13%)]\tLoss: 1.249765\n",
      "Train Epoch: 11 [5120/28821 (18%)]\tLoss: 1.248144\n",
      "Train Epoch: 11 [6400/28821 (22%)]\tLoss: 1.296950\n",
      "Train Epoch: 11 [7680/28821 (27%)]\tLoss: 1.341747\n",
      "Train Epoch: 11 [8960/28821 (31%)]\tLoss: 1.151960\n",
      "Train Epoch: 11 [10240/28821 (35%)]\tLoss: 1.399045\n",
      "Train Epoch: 11 [11520/28821 (40%)]\tLoss: 1.187816\n",
      "Train Epoch: 11 [12800/28821 (44%)]\tLoss: 1.364417\n",
      "Train Epoch: 11 [14080/28821 (49%)]\tLoss: 1.218498\n",
      "Train Epoch: 11 [15360/28821 (53%)]\tLoss: 1.188076\n",
      "Train Epoch: 11 [16640/28821 (58%)]\tLoss: 1.411304\n",
      "Train Epoch: 11 [17920/28821 (62%)]\tLoss: 1.176932\n",
      "Train Epoch: 11 [19200/28821 (67%)]\tLoss: 1.367309\n",
      "Train Epoch: 11 [20480/28821 (71%)]\tLoss: 1.324587\n",
      "Train Epoch: 11 [21760/28821 (75%)]\tLoss: 1.297073\n",
      "Train Epoch: 11 [23040/28821 (80%)]\tLoss: 1.172550\n",
      "Train Epoch: 11 [24320/28821 (84%)]\tLoss: 1.317076\n",
      "Train Epoch: 11 [25600/28821 (89%)]\tLoss: 1.604276\n",
      "Train Epoch: 11 [26880/28821 (93%)]\tLoss: 1.177139\n",
      "Train Epoch: 11 [28160/28821 (98%)]\tLoss: 1.417057\n",
      "Train Epoch: 12 [0/28821 (0%)]\tLoss: 1.521875\n",
      "Train Epoch: 12 [1280/28821 (4%)]\tLoss: 1.167798\n",
      "Train Epoch: 12 [2560/28821 (9%)]\tLoss: 1.264936\n",
      "Train Epoch: 12 [3840/28821 (13%)]\tLoss: 1.315168\n",
      "Train Epoch: 12 [5120/28821 (18%)]\tLoss: 1.457731\n",
      "Train Epoch: 12 [6400/28821 (22%)]\tLoss: 1.273312\n",
      "Train Epoch: 12 [7680/28821 (27%)]\tLoss: 1.140301\n",
      "Train Epoch: 12 [8960/28821 (31%)]\tLoss: 1.181923\n",
      "Train Epoch: 12 [10240/28821 (35%)]\tLoss: 1.339469\n",
      "Train Epoch: 12 [11520/28821 (40%)]\tLoss: 1.275215\n",
      "Train Epoch: 12 [12800/28821 (44%)]\tLoss: 1.437201\n",
      "Train Epoch: 12 [14080/28821 (49%)]\tLoss: 1.263797\n",
      "Train Epoch: 12 [15360/28821 (53%)]\tLoss: 1.228726\n",
      "Train Epoch: 12 [16640/28821 (58%)]\tLoss: 1.338579\n",
      "Train Epoch: 12 [17920/28821 (62%)]\tLoss: 1.164989\n",
      "Train Epoch: 12 [19200/28821 (67%)]\tLoss: 1.412493\n",
      "Train Epoch: 12 [20480/28821 (71%)]\tLoss: 1.261179\n",
      "Train Epoch: 12 [21760/28821 (75%)]\tLoss: 1.051413\n",
      "Train Epoch: 12 [23040/28821 (80%)]\tLoss: 1.421896\n",
      "Train Epoch: 12 [24320/28821 (84%)]\tLoss: 1.268494\n",
      "Train Epoch: 12 [25600/28821 (89%)]\tLoss: 1.341515\n",
      "Train Epoch: 12 [26880/28821 (93%)]\tLoss: 1.133748\n",
      "Train Epoch: 12 [28160/28821 (98%)]\tLoss: 1.233706\n",
      "Train Epoch: 13 [0/28821 (0%)]\tLoss: 1.382923\n",
      "Train Epoch: 13 [1280/28821 (4%)]\tLoss: 1.273786\n",
      "Train Epoch: 13 [2560/28821 (9%)]\tLoss: 1.369758\n",
      "Train Epoch: 13 [3840/28821 (13%)]\tLoss: 1.226200\n",
      "Train Epoch: 13 [5120/28821 (18%)]\tLoss: 1.274383\n",
      "Train Epoch: 13 [6400/28821 (22%)]\tLoss: 1.145551\n",
      "Train Epoch: 13 [7680/28821 (27%)]\tLoss: 1.317484\n",
      "Train Epoch: 13 [8960/28821 (31%)]\tLoss: 1.357327\n",
      "Train Epoch: 13 [10240/28821 (35%)]\tLoss: 1.418230\n",
      "Train Epoch: 13 [11520/28821 (40%)]\tLoss: 1.432926\n",
      "Train Epoch: 13 [12800/28821 (44%)]\tLoss: 1.356284\n",
      "Train Epoch: 13 [14080/28821 (49%)]\tLoss: 1.408671\n",
      "Train Epoch: 13 [15360/28821 (53%)]\tLoss: 1.264623\n",
      "Train Epoch: 13 [16640/28821 (58%)]\tLoss: 1.037043\n",
      "Train Epoch: 13 [17920/28821 (62%)]\tLoss: 1.312055\n",
      "Train Epoch: 13 [19200/28821 (67%)]\tLoss: 1.284145\n",
      "Train Epoch: 13 [20480/28821 (71%)]\tLoss: 1.335747\n",
      "Train Epoch: 13 [21760/28821 (75%)]\tLoss: 1.201934\n",
      "Train Epoch: 13 [23040/28821 (80%)]\tLoss: 1.353056\n",
      "Train Epoch: 13 [24320/28821 (84%)]\tLoss: 1.339672\n",
      "Train Epoch: 13 [25600/28821 (89%)]\tLoss: 1.250133\n",
      "Train Epoch: 13 [26880/28821 (93%)]\tLoss: 1.163035\n",
      "Train Epoch: 13 [28160/28821 (98%)]\tLoss: 1.181740\n",
      "Train Epoch: 14 [0/28821 (0%)]\tLoss: 1.289662\n",
      "Train Epoch: 14 [1280/28821 (4%)]\tLoss: 1.540109\n",
      "Train Epoch: 14 [2560/28821 (9%)]\tLoss: 1.384340\n",
      "Train Epoch: 14 [3840/28821 (13%)]\tLoss: 1.268224\n",
      "Train Epoch: 14 [5120/28821 (18%)]\tLoss: 1.269334\n",
      "Train Epoch: 14 [6400/28821 (22%)]\tLoss: 1.137850\n",
      "Train Epoch: 14 [7680/28821 (27%)]\tLoss: 1.441977\n",
      "Train Epoch: 14 [8960/28821 (31%)]\tLoss: 1.045554\n",
      "Train Epoch: 14 [10240/28821 (35%)]\tLoss: 1.235821\n",
      "Train Epoch: 14 [11520/28821 (40%)]\tLoss: 1.443860\n",
      "Train Epoch: 14 [12800/28821 (44%)]\tLoss: 1.348738\n",
      "Train Epoch: 14 [14080/28821 (49%)]\tLoss: 1.362459\n",
      "Train Epoch: 14 [15360/28821 (53%)]\tLoss: 1.218867\n",
      "Train Epoch: 14 [16640/28821 (58%)]\tLoss: 1.059280\n",
      "Train Epoch: 14 [17920/28821 (62%)]\tLoss: 1.211481\n",
      "Train Epoch: 14 [19200/28821 (67%)]\tLoss: 1.130141\n",
      "Train Epoch: 14 [20480/28821 (71%)]\tLoss: 1.148999\n",
      "Train Epoch: 14 [21760/28821 (75%)]\tLoss: 1.070003\n",
      "Train Epoch: 14 [23040/28821 (80%)]\tLoss: 1.213878\n",
      "Train Epoch: 14 [24320/28821 (84%)]\tLoss: 1.366496\n",
      "Train Epoch: 14 [25600/28821 (89%)]\tLoss: 1.397220\n",
      "Train Epoch: 14 [26880/28821 (93%)]\tLoss: 1.219097\n",
      "Train Epoch: 14 [28160/28821 (98%)]\tLoss: 1.191153\n",
      "Train Epoch: 15 [0/28821 (0%)]\tLoss: 1.293707\n",
      "Train Epoch: 15 [1280/28821 (4%)]\tLoss: 1.263083\n",
      "Train Epoch: 15 [2560/28821 (9%)]\tLoss: 1.367629\n",
      "Train Epoch: 15 [3840/28821 (13%)]\tLoss: 1.204644\n",
      "Train Epoch: 15 [5120/28821 (18%)]\tLoss: 1.179725\n",
      "Train Epoch: 15 [6400/28821 (22%)]\tLoss: 1.452492\n",
      "Train Epoch: 15 [7680/28821 (27%)]\tLoss: 1.170524\n",
      "Train Epoch: 15 [8960/28821 (31%)]\tLoss: 1.304724\n",
      "Train Epoch: 15 [10240/28821 (35%)]\tLoss: 1.336644\n",
      "Train Epoch: 15 [11520/28821 (40%)]\tLoss: 1.245479\n",
      "Train Epoch: 15 [12800/28821 (44%)]\tLoss: 1.208491\n",
      "Train Epoch: 15 [14080/28821 (49%)]\tLoss: 1.416530\n",
      "Train Epoch: 15 [15360/28821 (53%)]\tLoss: 1.401719\n",
      "Train Epoch: 15 [16640/28821 (58%)]\tLoss: 1.247894\n",
      "Train Epoch: 15 [17920/28821 (62%)]\tLoss: 1.251506\n",
      "Train Epoch: 15 [19200/28821 (67%)]\tLoss: 1.296999\n",
      "Train Epoch: 15 [20480/28821 (71%)]\tLoss: 1.341277\n",
      "Train Epoch: 15 [21760/28821 (75%)]\tLoss: 1.292124\n",
      "Train Epoch: 15 [23040/28821 (80%)]\tLoss: 1.201086\n",
      "Train Epoch: 15 [24320/28821 (84%)]\tLoss: 1.345192\n",
      "Train Epoch: 15 [25600/28821 (89%)]\tLoss: 1.087304\n",
      "Train Epoch: 15 [26880/28821 (93%)]\tLoss: 1.022443\n",
      "Train Epoch: 15 [28160/28821 (98%)]\tLoss: 1.297507\n",
      "Train Epoch: 16 [0/28821 (0%)]\tLoss: 1.296414\n",
      "Train Epoch: 16 [1280/28821 (4%)]\tLoss: 1.215131\n",
      "Train Epoch: 16 [2560/28821 (9%)]\tLoss: 1.119233\n",
      "Train Epoch: 16 [3840/28821 (13%)]\tLoss: 1.386557\n",
      "Train Epoch: 16 [5120/28821 (18%)]\tLoss: 1.318751\n",
      "Train Epoch: 16 [6400/28821 (22%)]\tLoss: 1.377392\n",
      "Train Epoch: 16 [7680/28821 (27%)]\tLoss: 1.346062\n",
      "Train Epoch: 16 [8960/28821 (31%)]\tLoss: 1.195861\n",
      "Train Epoch: 16 [10240/28821 (35%)]\tLoss: 1.253257\n",
      "Train Epoch: 16 [11520/28821 (40%)]\tLoss: 1.328096\n",
      "Train Epoch: 16 [12800/28821 (44%)]\tLoss: 1.182615\n",
      "Train Epoch: 16 [14080/28821 (49%)]\tLoss: 1.080157\n",
      "Train Epoch: 16 [15360/28821 (53%)]\tLoss: 1.431057\n",
      "Train Epoch: 16 [16640/28821 (58%)]\tLoss: 1.293447\n",
      "Train Epoch: 16 [17920/28821 (62%)]\tLoss: 1.059134\n",
      "Train Epoch: 16 [19200/28821 (67%)]\tLoss: 1.252418\n",
      "Train Epoch: 16 [20480/28821 (71%)]\tLoss: 1.256421\n",
      "Train Epoch: 16 [21760/28821 (75%)]\tLoss: 1.276905\n",
      "Train Epoch: 16 [23040/28821 (80%)]\tLoss: 1.470490\n",
      "Train Epoch: 16 [24320/28821 (84%)]\tLoss: 1.316528\n",
      "Train Epoch: 16 [25600/28821 (89%)]\tLoss: 1.213900\n",
      "Train Epoch: 16 [26880/28821 (93%)]\tLoss: 1.220394\n",
      "Train Epoch: 16 [28160/28821 (98%)]\tLoss: 1.110869\n",
      "Train Epoch: 17 [0/28821 (0%)]\tLoss: 1.290802\n",
      "Train Epoch: 17 [1280/28821 (4%)]\tLoss: 1.024049\n",
      "Train Epoch: 17 [2560/28821 (9%)]\tLoss: 1.420703\n",
      "Train Epoch: 17 [3840/28821 (13%)]\tLoss: 1.300718\n",
      "Train Epoch: 17 [5120/28821 (18%)]\tLoss: 1.313916\n",
      "Train Epoch: 17 [6400/28821 (22%)]\tLoss: 1.155387\n",
      "Train Epoch: 17 [7680/28821 (27%)]\tLoss: 1.336177\n",
      "Train Epoch: 17 [8960/28821 (31%)]\tLoss: 1.278509\n",
      "Train Epoch: 17 [10240/28821 (35%)]\tLoss: 1.466146\n",
      "Train Epoch: 17 [11520/28821 (40%)]\tLoss: 1.201541\n",
      "Train Epoch: 17 [12800/28821 (44%)]\tLoss: 1.227341\n",
      "Train Epoch: 17 [14080/28821 (49%)]\tLoss: 1.340351\n",
      "Train Epoch: 17 [15360/28821 (53%)]\tLoss: 1.437768\n",
      "Train Epoch: 17 [16640/28821 (58%)]\tLoss: 1.259790\n",
      "Train Epoch: 17 [17920/28821 (62%)]\tLoss: 1.309049\n",
      "Train Epoch: 17 [19200/28821 (67%)]\tLoss: 1.343970\n",
      "Train Epoch: 17 [20480/28821 (71%)]\tLoss: 1.193693\n",
      "Train Epoch: 17 [21760/28821 (75%)]\tLoss: 1.061789\n",
      "Train Epoch: 17 [23040/28821 (80%)]\tLoss: 1.185231\n",
      "Train Epoch: 17 [24320/28821 (84%)]\tLoss: 1.406523\n",
      "Train Epoch: 17 [25600/28821 (89%)]\tLoss: 1.214329\n",
      "Train Epoch: 17 [26880/28821 (93%)]\tLoss: 1.017085\n",
      "Train Epoch: 17 [28160/28821 (98%)]\tLoss: 1.289779\n",
      "Train Epoch: 18 [0/28821 (0%)]\tLoss: 1.297184\n",
      "Train Epoch: 18 [1280/28821 (4%)]\tLoss: 1.488725\n",
      "Train Epoch: 18 [2560/28821 (9%)]\tLoss: 1.180266\n",
      "Train Epoch: 18 [3840/28821 (13%)]\tLoss: 1.285006\n",
      "Train Epoch: 18 [5120/28821 (18%)]\tLoss: 1.120093\n",
      "Train Epoch: 18 [6400/28821 (22%)]\tLoss: 1.454590\n",
      "Train Epoch: 18 [7680/28821 (27%)]\tLoss: 1.483534\n",
      "Train Epoch: 18 [8960/28821 (31%)]\tLoss: 1.263672\n",
      "Train Epoch: 18 [10240/28821 (35%)]\tLoss: 1.193896\n",
      "Train Epoch: 18 [11520/28821 (40%)]\tLoss: 1.195379\n",
      "Train Epoch: 18 [12800/28821 (44%)]\tLoss: 1.227757\n",
      "Train Epoch: 18 [14080/28821 (49%)]\tLoss: 1.233469\n",
      "Train Epoch: 18 [15360/28821 (53%)]\tLoss: 1.212834\n",
      "Train Epoch: 18 [16640/28821 (58%)]\tLoss: 1.279064\n",
      "Train Epoch: 18 [17920/28821 (62%)]\tLoss: 1.161054\n",
      "Train Epoch: 18 [19200/28821 (67%)]\tLoss: 1.142366\n",
      "Train Epoch: 18 [20480/28821 (71%)]\tLoss: 1.221232\n",
      "Train Epoch: 18 [21760/28821 (75%)]\tLoss: 1.117988\n",
      "Train Epoch: 18 [23040/28821 (80%)]\tLoss: 1.221520\n",
      "Train Epoch: 18 [24320/28821 (84%)]\tLoss: 1.344187\n",
      "Train Epoch: 18 [25600/28821 (89%)]\tLoss: 1.309602\n",
      "Train Epoch: 18 [26880/28821 (93%)]\tLoss: 1.450570\n",
      "Train Epoch: 18 [28160/28821 (98%)]\tLoss: 1.128029\n",
      "Train Epoch: 19 [0/28821 (0%)]\tLoss: 1.021679\n",
      "Train Epoch: 19 [1280/28821 (4%)]\tLoss: 1.113438\n",
      "Train Epoch: 19 [2560/28821 (9%)]\tLoss: 1.260391\n",
      "Train Epoch: 19 [3840/28821 (13%)]\tLoss: 1.135555\n",
      "Train Epoch: 19 [5120/28821 (18%)]\tLoss: 1.482784\n",
      "Train Epoch: 19 [6400/28821 (22%)]\tLoss: 1.245202\n",
      "Train Epoch: 19 [7680/28821 (27%)]\tLoss: 0.999626\n",
      "Train Epoch: 19 [8960/28821 (31%)]\tLoss: 1.358398\n",
      "Train Epoch: 19 [10240/28821 (35%)]\tLoss: 1.280517\n",
      "Train Epoch: 19 [11520/28821 (40%)]\tLoss: 1.323004\n",
      "Train Epoch: 19 [12800/28821 (44%)]\tLoss: 1.331033\n",
      "Train Epoch: 19 [14080/28821 (49%)]\tLoss: 1.445092\n",
      "Train Epoch: 19 [15360/28821 (53%)]\tLoss: 1.360085\n",
      "Train Epoch: 19 [16640/28821 (58%)]\tLoss: 1.377456\n",
      "Train Epoch: 19 [17920/28821 (62%)]\tLoss: 1.412366\n",
      "Train Epoch: 19 [19200/28821 (67%)]\tLoss: 1.357190\n",
      "Train Epoch: 19 [20480/28821 (71%)]\tLoss: 1.237280\n",
      "Train Epoch: 19 [21760/28821 (75%)]\tLoss: 1.219117\n",
      "Train Epoch: 19 [23040/28821 (80%)]\tLoss: 1.165707\n",
      "Train Epoch: 19 [24320/28821 (84%)]\tLoss: 1.242321\n",
      "Train Epoch: 19 [25600/28821 (89%)]\tLoss: 1.148949\n",
      "Train Epoch: 19 [26880/28821 (93%)]\tLoss: 1.348230\n",
      "Train Epoch: 19 [28160/28821 (98%)]\tLoss: 1.213246\n",
      "Train Epoch: 20 [0/28821 (0%)]\tLoss: 1.164288\n",
      "Train Epoch: 20 [1280/28821 (4%)]\tLoss: 1.367104\n",
      "Train Epoch: 20 [2560/28821 (9%)]\tLoss: 1.096440\n",
      "Train Epoch: 20 [3840/28821 (13%)]\tLoss: 1.284121\n",
      "Train Epoch: 20 [5120/28821 (18%)]\tLoss: 1.240221\n",
      "Train Epoch: 20 [6400/28821 (22%)]\tLoss: 1.147407\n",
      "Train Epoch: 20 [7680/28821 (27%)]\tLoss: 1.475390\n",
      "Train Epoch: 20 [8960/28821 (31%)]\tLoss: 1.069083\n",
      "Train Epoch: 20 [10240/28821 (35%)]\tLoss: 1.024395\n",
      "Train Epoch: 20 [11520/28821 (40%)]\tLoss: 1.101005\n",
      "Train Epoch: 20 [12800/28821 (44%)]\tLoss: 1.211408\n",
      "Train Epoch: 20 [14080/28821 (49%)]\tLoss: 1.322102\n",
      "Train Epoch: 20 [15360/28821 (53%)]\tLoss: 1.198135\n",
      "Train Epoch: 20 [16640/28821 (58%)]\tLoss: 1.281670\n",
      "Train Epoch: 20 [17920/28821 (62%)]\tLoss: 1.407283\n",
      "Train Epoch: 20 [19200/28821 (67%)]\tLoss: 1.159897\n",
      "Train Epoch: 20 [20480/28821 (71%)]\tLoss: 1.240980\n",
      "Train Epoch: 20 [21760/28821 (75%)]\tLoss: 1.184588\n",
      "Train Epoch: 20 [23040/28821 (80%)]\tLoss: 1.293722\n",
      "Train Epoch: 20 [24320/28821 (84%)]\tLoss: 1.343780\n",
      "Train Epoch: 20 [25600/28821 (89%)]\tLoss: 1.176803\n",
      "Train Epoch: 20 [26880/28821 (93%)]\tLoss: 1.283015\n",
      "Train Epoch: 20 [28160/28821 (98%)]\tLoss: 1.090008\n",
      "Train Epoch: 21 [0/28821 (0%)]\tLoss: 1.118868\n",
      "Train Epoch: 21 [1280/28821 (4%)]\tLoss: 1.398223\n",
      "Train Epoch: 21 [2560/28821 (9%)]\tLoss: 1.343728\n",
      "Train Epoch: 21 [3840/28821 (13%)]\tLoss: 1.158561\n",
      "Train Epoch: 21 [5120/28821 (18%)]\tLoss: 1.164779\n",
      "Train Epoch: 21 [6400/28821 (22%)]\tLoss: 1.408254\n",
      "Train Epoch: 21 [7680/28821 (27%)]\tLoss: 1.393658\n",
      "Train Epoch: 21 [8960/28821 (31%)]\tLoss: 1.276039\n",
      "Train Epoch: 21 [10240/28821 (35%)]\tLoss: 1.093620\n",
      "Train Epoch: 21 [11520/28821 (40%)]\tLoss: 1.080866\n",
      "Train Epoch: 21 [12800/28821 (44%)]\tLoss: 1.112321\n",
      "Train Epoch: 21 [14080/28821 (49%)]\tLoss: 1.201147\n",
      "Train Epoch: 21 [15360/28821 (53%)]\tLoss: 1.346395\n",
      "Train Epoch: 21 [16640/28821 (58%)]\tLoss: 1.180482\n",
      "Train Epoch: 21 [17920/28821 (62%)]\tLoss: 1.162987\n",
      "Train Epoch: 21 [19200/28821 (67%)]\tLoss: 1.253737\n",
      "Train Epoch: 21 [20480/28821 (71%)]\tLoss: 1.008518\n",
      "Train Epoch: 21 [21760/28821 (75%)]\tLoss: 1.174443\n",
      "Train Epoch: 21 [23040/28821 (80%)]\tLoss: 1.178655\n",
      "Train Epoch: 21 [24320/28821 (84%)]\tLoss: 1.098071\n",
      "Train Epoch: 21 [25600/28821 (89%)]\tLoss: 1.347057\n",
      "Train Epoch: 21 [26880/28821 (93%)]\tLoss: 1.154204\n",
      "Train Epoch: 21 [28160/28821 (98%)]\tLoss: 1.051135\n",
      "Train Epoch: 22 [0/28821 (0%)]\tLoss: 1.313833\n",
      "Train Epoch: 22 [1280/28821 (4%)]\tLoss: 1.373222\n",
      "Train Epoch: 22 [2560/28821 (9%)]\tLoss: 1.146501\n",
      "Train Epoch: 22 [3840/28821 (13%)]\tLoss: 1.154080\n",
      "Train Epoch: 22 [5120/28821 (18%)]\tLoss: 1.026015\n",
      "Train Epoch: 22 [6400/28821 (22%)]\tLoss: 1.349410\n",
      "Train Epoch: 22 [7680/28821 (27%)]\tLoss: 1.296263\n",
      "Train Epoch: 22 [8960/28821 (31%)]\tLoss: 1.170548\n",
      "Train Epoch: 22 [10240/28821 (35%)]\tLoss: 0.984748\n",
      "Train Epoch: 22 [11520/28821 (40%)]\tLoss: 1.049519\n",
      "Train Epoch: 22 [12800/28821 (44%)]\tLoss: 1.221542\n",
      "Train Epoch: 22 [14080/28821 (49%)]\tLoss: 1.195965\n",
      "Train Epoch: 22 [15360/28821 (53%)]\tLoss: 1.184894\n",
      "Train Epoch: 22 [16640/28821 (58%)]\tLoss: 1.068235\n",
      "Train Epoch: 22 [17920/28821 (62%)]\tLoss: 1.235945\n",
      "Train Epoch: 22 [19200/28821 (67%)]\tLoss: 1.082590\n",
      "Train Epoch: 22 [20480/28821 (71%)]\tLoss: 1.138976\n",
      "Train Epoch: 22 [21760/28821 (75%)]\tLoss: 1.175225\n",
      "Train Epoch: 22 [23040/28821 (80%)]\tLoss: 1.033500\n",
      "Train Epoch: 22 [24320/28821 (84%)]\tLoss: 1.002992\n",
      "Train Epoch: 22 [25600/28821 (89%)]\tLoss: 1.059540\n",
      "Train Epoch: 22 [26880/28821 (93%)]\tLoss: 1.074580\n",
      "Train Epoch: 22 [28160/28821 (98%)]\tLoss: 1.211389\n",
      "Train Epoch: 23 [0/28821 (0%)]\tLoss: 1.023892\n",
      "Train Epoch: 23 [1280/28821 (4%)]\tLoss: 1.057887\n",
      "Train Epoch: 23 [2560/28821 (9%)]\tLoss: 1.305433\n",
      "Train Epoch: 23 [3840/28821 (13%)]\tLoss: 1.021449\n",
      "Train Epoch: 23 [5120/28821 (18%)]\tLoss: 1.080552\n",
      "Train Epoch: 23 [6400/28821 (22%)]\tLoss: 1.151851\n",
      "Train Epoch: 23 [7680/28821 (27%)]\tLoss: 1.238142\n",
      "Train Epoch: 23 [8960/28821 (31%)]\tLoss: 1.063243\n",
      "Train Epoch: 23 [10240/28821 (35%)]\tLoss: 0.999936\n",
      "Train Epoch: 23 [11520/28821 (40%)]\tLoss: 1.335453\n",
      "Train Epoch: 23 [12800/28821 (44%)]\tLoss: 1.062649\n",
      "Train Epoch: 23 [14080/28821 (49%)]\tLoss: 1.149480\n",
      "Train Epoch: 23 [15360/28821 (53%)]\tLoss: 0.928116\n",
      "Train Epoch: 23 [16640/28821 (58%)]\tLoss: 1.130434\n",
      "Train Epoch: 23 [17920/28821 (62%)]\tLoss: 1.106641\n",
      "Train Epoch: 23 [19200/28821 (67%)]\tLoss: 1.246161\n",
      "Train Epoch: 23 [20480/28821 (71%)]\tLoss: 0.963098\n",
      "Train Epoch: 23 [21760/28821 (75%)]\tLoss: 1.342718\n",
      "Train Epoch: 23 [23040/28821 (80%)]\tLoss: 1.075469\n",
      "Train Epoch: 23 [24320/28821 (84%)]\tLoss: 1.214523\n",
      "Train Epoch: 23 [25600/28821 (89%)]\tLoss: 0.983538\n",
      "Train Epoch: 23 [26880/28821 (93%)]\tLoss: 1.064680\n",
      "Train Epoch: 23 [28160/28821 (98%)]\tLoss: 1.171068\n",
      "Train Epoch: 24 [0/28821 (0%)]\tLoss: 1.140386\n",
      "Train Epoch: 24 [1280/28821 (4%)]\tLoss: 1.185182\n",
      "Train Epoch: 24 [2560/28821 (9%)]\tLoss: 1.067165\n",
      "Train Epoch: 24 [3840/28821 (13%)]\tLoss: 1.047369\n",
      "Train Epoch: 24 [5120/28821 (18%)]\tLoss: 1.398027\n",
      "Train Epoch: 24 [6400/28821 (22%)]\tLoss: 1.143256\n",
      "Train Epoch: 24 [7680/28821 (27%)]\tLoss: 1.111524\n",
      "Train Epoch: 24 [8960/28821 (31%)]\tLoss: 1.157388\n",
      "Train Epoch: 24 [10240/28821 (35%)]\tLoss: 1.169411\n",
      "Train Epoch: 24 [11520/28821 (40%)]\tLoss: 1.105560\n",
      "Train Epoch: 24 [12800/28821 (44%)]\tLoss: 1.252108\n",
      "Train Epoch: 24 [14080/28821 (49%)]\tLoss: 1.505641\n",
      "Train Epoch: 24 [15360/28821 (53%)]\tLoss: 1.399257\n",
      "Train Epoch: 24 [16640/28821 (58%)]\tLoss: 1.168988\n",
      "Train Epoch: 24 [17920/28821 (62%)]\tLoss: 1.342502\n",
      "Train Epoch: 24 [19200/28821 (67%)]\tLoss: 1.103281\n",
      "Train Epoch: 24 [20480/28821 (71%)]\tLoss: 1.341419\n",
      "Train Epoch: 24 [21760/28821 (75%)]\tLoss: 1.394264\n",
      "Train Epoch: 24 [23040/28821 (80%)]\tLoss: 1.148156\n",
      "Train Epoch: 24 [24320/28821 (84%)]\tLoss: 1.295787\n",
      "Train Epoch: 24 [25600/28821 (89%)]\tLoss: 1.133774\n",
      "Train Epoch: 24 [26880/28821 (93%)]\tLoss: 1.108277\n",
      "Train Epoch: 24 [28160/28821 (98%)]\tLoss: 0.975965\n",
      "Train Epoch: 25 [0/28821 (0%)]\tLoss: 1.042474\n",
      "Train Epoch: 25 [1280/28821 (4%)]\tLoss: 1.020499\n",
      "Train Epoch: 25 [2560/28821 (9%)]\tLoss: 1.109124\n",
      "Train Epoch: 25 [3840/28821 (13%)]\tLoss: 1.522668\n",
      "Train Epoch: 25 [5120/28821 (18%)]\tLoss: 1.160846\n",
      "Train Epoch: 25 [6400/28821 (22%)]\tLoss: 1.028924\n",
      "Train Epoch: 25 [7680/28821 (27%)]\tLoss: 1.322727\n",
      "Train Epoch: 25 [8960/28821 (31%)]\tLoss: 1.124199\n",
      "Train Epoch: 25 [10240/28821 (35%)]\tLoss: 1.202273\n",
      "Train Epoch: 25 [11520/28821 (40%)]\tLoss: 1.007304\n",
      "Train Epoch: 25 [12800/28821 (44%)]\tLoss: 1.445921\n",
      "Train Epoch: 25 [14080/28821 (49%)]\tLoss: 1.341666\n",
      "Train Epoch: 25 [15360/28821 (53%)]\tLoss: 1.242457\n",
      "Train Epoch: 25 [16640/28821 (58%)]\tLoss: 1.111265\n",
      "Train Epoch: 25 [17920/28821 (62%)]\tLoss: 1.304996\n",
      "Train Epoch: 25 [19200/28821 (67%)]\tLoss: 1.285978\n",
      "Train Epoch: 25 [20480/28821 (71%)]\tLoss: 1.138423\n",
      "Train Epoch: 25 [21760/28821 (75%)]\tLoss: 1.128275\n",
      "Train Epoch: 25 [23040/28821 (80%)]\tLoss: 1.066397\n",
      "Train Epoch: 25 [24320/28821 (84%)]\tLoss: 1.286527\n",
      "Train Epoch: 25 [25600/28821 (89%)]\tLoss: 1.045382\n",
      "Train Epoch: 25 [26880/28821 (93%)]\tLoss: 1.084801\n",
      "Train Epoch: 25 [28160/28821 (98%)]\tLoss: 1.363359\n",
      "Train Epoch: 26 [0/28821 (0%)]\tLoss: 1.277826\n",
      "Train Epoch: 26 [1280/28821 (4%)]\tLoss: 1.338237\n",
      "Train Epoch: 26 [2560/28821 (9%)]\tLoss: 1.111040\n",
      "Train Epoch: 26 [3840/28821 (13%)]\tLoss: 1.306160\n",
      "Train Epoch: 26 [5120/28821 (18%)]\tLoss: 1.262269\n",
      "Train Epoch: 26 [6400/28821 (22%)]\tLoss: 1.157374\n",
      "Train Epoch: 26 [7680/28821 (27%)]\tLoss: 1.102205\n",
      "Train Epoch: 26 [8960/28821 (31%)]\tLoss: 1.268553\n",
      "Train Epoch: 26 [10240/28821 (35%)]\tLoss: 1.084927\n",
      "Train Epoch: 26 [11520/28821 (40%)]\tLoss: 0.924715\n",
      "Train Epoch: 26 [12800/28821 (44%)]\tLoss: 1.170612\n",
      "Train Epoch: 26 [14080/28821 (49%)]\tLoss: 1.294916\n",
      "Train Epoch: 26 [15360/28821 (53%)]\tLoss: 1.178189\n",
      "Train Epoch: 26 [16640/28821 (58%)]\tLoss: 1.219476\n",
      "Train Epoch: 26 [17920/28821 (62%)]\tLoss: 1.264104\n",
      "Train Epoch: 26 [19200/28821 (67%)]\tLoss: 1.128511\n",
      "Train Epoch: 26 [20480/28821 (71%)]\tLoss: 1.165432\n",
      "Train Epoch: 26 [21760/28821 (75%)]\tLoss: 1.048022\n",
      "Train Epoch: 26 [23040/28821 (80%)]\tLoss: 1.373552\n",
      "Train Epoch: 26 [24320/28821 (84%)]\tLoss: 1.053101\n",
      "Train Epoch: 26 [25600/28821 (89%)]\tLoss: 1.406684\n",
      "Train Epoch: 26 [26880/28821 (93%)]\tLoss: 1.391054\n",
      "Train Epoch: 26 [28160/28821 (98%)]\tLoss: 1.199763\n",
      "Train Epoch: 27 [0/28821 (0%)]\tLoss: 1.202029\n",
      "Train Epoch: 27 [1280/28821 (4%)]\tLoss: 1.276879\n",
      "Train Epoch: 27 [2560/28821 (9%)]\tLoss: 1.149196\n",
      "Train Epoch: 27 [3840/28821 (13%)]\tLoss: 1.239727\n",
      "Train Epoch: 27 [5120/28821 (18%)]\tLoss: 1.178899\n",
      "Train Epoch: 27 [6400/28821 (22%)]\tLoss: 1.489515\n",
      "Train Epoch: 27 [7680/28821 (27%)]\tLoss: 1.438233\n",
      "Train Epoch: 27 [8960/28821 (31%)]\tLoss: 1.221003\n",
      "Train Epoch: 27 [10240/28821 (35%)]\tLoss: 0.951985\n",
      "Train Epoch: 27 [11520/28821 (40%)]\tLoss: 1.334673\n",
      "Train Epoch: 27 [12800/28821 (44%)]\tLoss: 1.222241\n",
      "Train Epoch: 27 [14080/28821 (49%)]\tLoss: 1.047738\n",
      "Train Epoch: 27 [15360/28821 (53%)]\tLoss: 1.037428\n",
      "Train Epoch: 27 [16640/28821 (58%)]\tLoss: 1.279944\n",
      "Train Epoch: 27 [17920/28821 (62%)]\tLoss: 1.042506\n",
      "Train Epoch: 27 [19200/28821 (67%)]\tLoss: 1.273444\n",
      "Train Epoch: 27 [20480/28821 (71%)]\tLoss: 1.452445\n",
      "Train Epoch: 27 [21760/28821 (75%)]\tLoss: 1.102038\n",
      "Train Epoch: 27 [23040/28821 (80%)]\tLoss: 1.279214\n",
      "Train Epoch: 27 [24320/28821 (84%)]\tLoss: 1.192757\n",
      "Train Epoch: 27 [25600/28821 (89%)]\tLoss: 1.108678\n",
      "Train Epoch: 27 [26880/28821 (93%)]\tLoss: 1.039714\n",
      "Train Epoch: 27 [28160/28821 (98%)]\tLoss: 1.538853\n",
      "Train Epoch: 28 [0/28821 (0%)]\tLoss: 1.152350\n",
      "Train Epoch: 28 [1280/28821 (4%)]\tLoss: 0.978550\n",
      "Train Epoch: 28 [2560/28821 (9%)]\tLoss: 1.197850\n",
      "Train Epoch: 28 [3840/28821 (13%)]\tLoss: 1.131111\n",
      "Train Epoch: 28 [5120/28821 (18%)]\tLoss: 1.027500\n",
      "Train Epoch: 28 [6400/28821 (22%)]\tLoss: 1.316016\n",
      "Train Epoch: 28 [7680/28821 (27%)]\tLoss: 1.142306\n",
      "Train Epoch: 28 [8960/28821 (31%)]\tLoss: 1.282445\n",
      "Train Epoch: 28 [10240/28821 (35%)]\tLoss: 1.346986\n",
      "Train Epoch: 28 [11520/28821 (40%)]\tLoss: 1.195542\n",
      "Train Epoch: 28 [12800/28821 (44%)]\tLoss: 1.185244\n",
      "Train Epoch: 28 [14080/28821 (49%)]\tLoss: 1.262706\n",
      "Train Epoch: 28 [15360/28821 (53%)]\tLoss: 1.112672\n",
      "Train Epoch: 28 [16640/28821 (58%)]\tLoss: 1.217231\n",
      "Train Epoch: 28 [17920/28821 (62%)]\tLoss: 1.118029\n",
      "Train Epoch: 28 [19200/28821 (67%)]\tLoss: 1.159452\n",
      "Train Epoch: 28 [20480/28821 (71%)]\tLoss: 1.166711\n",
      "Train Epoch: 28 [21760/28821 (75%)]\tLoss: 1.240551\n",
      "Train Epoch: 28 [23040/28821 (80%)]\tLoss: 1.127998\n",
      "Train Epoch: 28 [24320/28821 (84%)]\tLoss: 1.227645\n",
      "Train Epoch: 28 [25600/28821 (89%)]\tLoss: 1.123807\n",
      "Train Epoch: 28 [26880/28821 (93%)]\tLoss: 1.120596\n",
      "Train Epoch: 28 [28160/28821 (98%)]\tLoss: 1.176766\n",
      "Train Epoch: 29 [0/28821 (0%)]\tLoss: 1.369131\n",
      "Train Epoch: 29 [1280/28821 (4%)]\tLoss: 1.193428\n",
      "Train Epoch: 29 [2560/28821 (9%)]\tLoss: 1.220226\n",
      "Train Epoch: 29 [3840/28821 (13%)]\tLoss: 1.152478\n",
      "Train Epoch: 29 [5120/28821 (18%)]\tLoss: 1.201537\n",
      "Train Epoch: 29 [6400/28821 (22%)]\tLoss: 1.077307\n",
      "Train Epoch: 29 [7680/28821 (27%)]\tLoss: 1.326162\n",
      "Train Epoch: 29 [8960/28821 (31%)]\tLoss: 1.135291\n",
      "Train Epoch: 29 [10240/28821 (35%)]\tLoss: 1.198514\n",
      "Train Epoch: 29 [11520/28821 (40%)]\tLoss: 1.233260\n",
      "Train Epoch: 29 [12800/28821 (44%)]\tLoss: 1.129708\n",
      "Train Epoch: 29 [14080/28821 (49%)]\tLoss: 1.341045\n",
      "Train Epoch: 29 [15360/28821 (53%)]\tLoss: 1.321645\n",
      "Train Epoch: 29 [16640/28821 (58%)]\tLoss: 1.098574\n",
      "Train Epoch: 29 [17920/28821 (62%)]\tLoss: 1.282812\n",
      "Train Epoch: 29 [19200/28821 (67%)]\tLoss: 1.186538\n",
      "Train Epoch: 29 [20480/28821 (71%)]\tLoss: 1.249996\n",
      "Train Epoch: 29 [21760/28821 (75%)]\tLoss: 1.188629\n",
      "Train Epoch: 29 [23040/28821 (80%)]\tLoss: 1.121469\n",
      "Train Epoch: 29 [24320/28821 (84%)]\tLoss: 1.276918\n",
      "Train Epoch: 29 [25600/28821 (89%)]\tLoss: 1.128161\n",
      "Train Epoch: 29 [26880/28821 (93%)]\tLoss: 1.162706\n",
      "Train Epoch: 29 [28160/28821 (98%)]\tLoss: 1.205340\n",
      "Train Epoch: 30 [0/28821 (0%)]\tLoss: 1.113466\n",
      "Train Epoch: 30 [1280/28821 (4%)]\tLoss: 1.211739\n",
      "Train Epoch: 30 [2560/28821 (9%)]\tLoss: 1.334820\n",
      "Train Epoch: 30 [3840/28821 (13%)]\tLoss: 1.264946\n",
      "Train Epoch: 30 [5120/28821 (18%)]\tLoss: 1.190984\n",
      "Train Epoch: 30 [6400/28821 (22%)]\tLoss: 1.168920\n",
      "Train Epoch: 30 [7680/28821 (27%)]\tLoss: 1.229943\n",
      "Train Epoch: 30 [8960/28821 (31%)]\tLoss: 1.247105\n",
      "Train Epoch: 30 [10240/28821 (35%)]\tLoss: 1.156221\n",
      "Train Epoch: 30 [11520/28821 (40%)]\tLoss: 1.207611\n",
      "Train Epoch: 30 [12800/28821 (44%)]\tLoss: 1.028230\n",
      "Train Epoch: 30 [14080/28821 (49%)]\tLoss: 1.105201\n",
      "Train Epoch: 30 [15360/28821 (53%)]\tLoss: 1.256330\n",
      "Train Epoch: 30 [16640/28821 (58%)]\tLoss: 1.112898\n",
      "Train Epoch: 30 [17920/28821 (62%)]\tLoss: 1.167351\n",
      "Train Epoch: 30 [19200/28821 (67%)]\tLoss: 1.110066\n",
      "Train Epoch: 30 [20480/28821 (71%)]\tLoss: 1.326009\n",
      "Train Epoch: 30 [21760/28821 (75%)]\tLoss: 1.228002\n",
      "Train Epoch: 30 [23040/28821 (80%)]\tLoss: 1.258012\n",
      "Train Epoch: 30 [24320/28821 (84%)]\tLoss: 1.503306\n",
      "Train Epoch: 30 [25600/28821 (89%)]\tLoss: 1.049890\n",
      "Train Epoch: 30 [26880/28821 (93%)]\tLoss: 1.136787\n",
      "Train Epoch: 30 [28160/28821 (98%)]\tLoss: 1.402064\n",
      "Train Epoch: 31 [0/28821 (0%)]\tLoss: 1.211722\n",
      "Train Epoch: 31 [1280/28821 (4%)]\tLoss: 1.198427\n",
      "Train Epoch: 31 [2560/28821 (9%)]\tLoss: 1.085796\n",
      "Train Epoch: 31 [3840/28821 (13%)]\tLoss: 1.156433\n",
      "Train Epoch: 31 [5120/28821 (18%)]\tLoss: 1.150798\n",
      "Train Epoch: 31 [6400/28821 (22%)]\tLoss: 1.213132\n",
      "Train Epoch: 31 [7680/28821 (27%)]\tLoss: 1.163264\n",
      "Train Epoch: 31 [8960/28821 (31%)]\tLoss: 1.009361\n",
      "Train Epoch: 31 [10240/28821 (35%)]\tLoss: 1.317314\n",
      "Train Epoch: 31 [11520/28821 (40%)]\tLoss: 1.184274\n",
      "Train Epoch: 31 [12800/28821 (44%)]\tLoss: 1.117796\n",
      "Train Epoch: 31 [14080/28821 (49%)]\tLoss: 1.175773\n",
      "Train Epoch: 31 [15360/28821 (53%)]\tLoss: 1.185046\n",
      "Train Epoch: 31 [16640/28821 (58%)]\tLoss: 1.329769\n",
      "Train Epoch: 31 [17920/28821 (62%)]\tLoss: 1.176393\n",
      "Train Epoch: 31 [19200/28821 (67%)]\tLoss: 1.157636\n",
      "Train Epoch: 31 [20480/28821 (71%)]\tLoss: 1.224867\n",
      "Train Epoch: 31 [21760/28821 (75%)]\tLoss: 1.165832\n",
      "Train Epoch: 31 [23040/28821 (80%)]\tLoss: 1.061926\n",
      "Train Epoch: 31 [24320/28821 (84%)]\tLoss: 1.266627\n",
      "Train Epoch: 31 [25600/28821 (89%)]\tLoss: 1.090011\n",
      "Train Epoch: 31 [26880/28821 (93%)]\tLoss: 1.235377\n",
      "Train Epoch: 31 [28160/28821 (98%)]\tLoss: 1.147590\n",
      "Train Epoch: 32 [0/28821 (0%)]\tLoss: 1.150443\n",
      "Train Epoch: 32 [1280/28821 (4%)]\tLoss: 1.330423\n",
      "Train Epoch: 32 [2560/28821 (9%)]\tLoss: 1.164773\n",
      "Train Epoch: 32 [3840/28821 (13%)]\tLoss: 1.224169\n",
      "Train Epoch: 32 [5120/28821 (18%)]\tLoss: 0.975589\n",
      "Train Epoch: 32 [6400/28821 (22%)]\tLoss: 1.123501\n",
      "Train Epoch: 32 [7680/28821 (27%)]\tLoss: 1.262547\n",
      "Train Epoch: 32 [8960/28821 (31%)]\tLoss: 1.214438\n",
      "Train Epoch: 32 [10240/28821 (35%)]\tLoss: 1.183402\n",
      "Train Epoch: 32 [11520/28821 (40%)]\tLoss: 1.106246\n",
      "Train Epoch: 32 [12800/28821 (44%)]\tLoss: 1.089678\n",
      "Train Epoch: 32 [14080/28821 (49%)]\tLoss: 1.244550\n",
      "Train Epoch: 32 [15360/28821 (53%)]\tLoss: 1.216735\n",
      "Train Epoch: 32 [16640/28821 (58%)]\tLoss: 1.443871\n",
      "Train Epoch: 32 [17920/28821 (62%)]\tLoss: 1.113327\n",
      "Train Epoch: 32 [19200/28821 (67%)]\tLoss: 1.265493\n",
      "Train Epoch: 32 [20480/28821 (71%)]\tLoss: 1.112936\n",
      "Train Epoch: 32 [21760/28821 (75%)]\tLoss: 1.459208\n",
      "Train Epoch: 32 [23040/28821 (80%)]\tLoss: 1.478594\n",
      "Train Epoch: 32 [24320/28821 (84%)]\tLoss: 1.244166\n",
      "Train Epoch: 32 [25600/28821 (89%)]\tLoss: 1.236137\n",
      "Train Epoch: 32 [26880/28821 (93%)]\tLoss: 1.089728\n",
      "Train Epoch: 32 [28160/28821 (98%)]\tLoss: 1.199234\n",
      "Train Epoch: 33 [0/28821 (0%)]\tLoss: 1.285387\n",
      "Train Epoch: 33 [1280/28821 (4%)]\tLoss: 1.100595\n",
      "Train Epoch: 33 [2560/28821 (9%)]\tLoss: 1.071241\n",
      "Train Epoch: 33 [3840/28821 (13%)]\tLoss: 0.949046\n",
      "Train Epoch: 33 [5120/28821 (18%)]\tLoss: 1.228294\n",
      "Train Epoch: 33 [6400/28821 (22%)]\tLoss: 1.196825\n",
      "Train Epoch: 33 [7680/28821 (27%)]\tLoss: 1.187605\n",
      "Train Epoch: 33 [8960/28821 (31%)]\tLoss: 1.132705\n",
      "Train Epoch: 33 [10240/28821 (35%)]\tLoss: 0.942542\n",
      "Train Epoch: 33 [11520/28821 (40%)]\tLoss: 1.043064\n",
      "Train Epoch: 33 [12800/28821 (44%)]\tLoss: 1.131901\n",
      "Train Epoch: 33 [14080/28821 (49%)]\tLoss: 1.271881\n",
      "Train Epoch: 33 [15360/28821 (53%)]\tLoss: 1.391435\n",
      "Train Epoch: 33 [16640/28821 (58%)]\tLoss: 1.099616\n",
      "Train Epoch: 33 [17920/28821 (62%)]\tLoss: 1.205913\n",
      "Train Epoch: 33 [19200/28821 (67%)]\tLoss: 1.257379\n",
      "Train Epoch: 33 [20480/28821 (71%)]\tLoss: 1.065870\n",
      "Train Epoch: 33 [21760/28821 (75%)]\tLoss: 1.125840\n",
      "Train Epoch: 33 [23040/28821 (80%)]\tLoss: 1.140047\n",
      "Train Epoch: 33 [24320/28821 (84%)]\tLoss: 1.125694\n",
      "Train Epoch: 33 [25600/28821 (89%)]\tLoss: 1.229924\n",
      "Train Epoch: 33 [26880/28821 (93%)]\tLoss: 1.208615\n",
      "Train Epoch: 33 [28160/28821 (98%)]\tLoss: 1.057093\n",
      "Train Epoch: 34 [0/28821 (0%)]\tLoss: 1.138694\n",
      "Train Epoch: 34 [1280/28821 (4%)]\tLoss: 1.043753\n",
      "Train Epoch: 34 [2560/28821 (9%)]\tLoss: 0.989402\n",
      "Train Epoch: 34 [3840/28821 (13%)]\tLoss: 1.273780\n",
      "Train Epoch: 34 [5120/28821 (18%)]\tLoss: 1.349996\n",
      "Train Epoch: 34 [6400/28821 (22%)]\tLoss: 1.174644\n",
      "Train Epoch: 34 [7680/28821 (27%)]\tLoss: 1.254589\n",
      "Train Epoch: 34 [8960/28821 (31%)]\tLoss: 1.179462\n",
      "Train Epoch: 34 [10240/28821 (35%)]\tLoss: 1.205933\n",
      "Train Epoch: 34 [11520/28821 (40%)]\tLoss: 1.463004\n",
      "Train Epoch: 34 [12800/28821 (44%)]\tLoss: 1.153268\n",
      "Train Epoch: 34 [14080/28821 (49%)]\tLoss: 1.188296\n",
      "Train Epoch: 34 [15360/28821 (53%)]\tLoss: 1.208225\n",
      "Train Epoch: 34 [16640/28821 (58%)]\tLoss: 1.026890\n",
      "Train Epoch: 34 [17920/28821 (62%)]\tLoss: 1.143178\n",
      "Train Epoch: 34 [19200/28821 (67%)]\tLoss: 1.201409\n",
      "Train Epoch: 34 [20480/28821 (71%)]\tLoss: 0.893485\n",
      "Train Epoch: 34 [21760/28821 (75%)]\tLoss: 1.083747\n",
      "Train Epoch: 34 [23040/28821 (80%)]\tLoss: 1.153625\n",
      "Train Epoch: 34 [24320/28821 (84%)]\tLoss: 1.237342\n",
      "Train Epoch: 34 [25600/28821 (89%)]\tLoss: 0.936049\n",
      "Train Epoch: 34 [26880/28821 (93%)]\tLoss: 1.147562\n",
      "Train Epoch: 34 [28160/28821 (98%)]\tLoss: 1.192119\n",
      "Train Epoch: 35 [0/28821 (0%)]\tLoss: 1.251874\n",
      "Train Epoch: 35 [1280/28821 (4%)]\tLoss: 1.094337\n",
      "Train Epoch: 35 [2560/28821 (9%)]\tLoss: 1.284663\n",
      "Train Epoch: 35 [3840/28821 (13%)]\tLoss: 1.209463\n",
      "Train Epoch: 35 [5120/28821 (18%)]\tLoss: 1.216784\n",
      "Train Epoch: 35 [6400/28821 (22%)]\tLoss: 1.049423\n",
      "Train Epoch: 35 [7680/28821 (27%)]\tLoss: 1.084767\n",
      "Train Epoch: 35 [8960/28821 (31%)]\tLoss: 1.058548\n",
      "Train Epoch: 35 [10240/28821 (35%)]\tLoss: 1.091476\n",
      "Train Epoch: 35 [11520/28821 (40%)]\tLoss: 1.208233\n",
      "Train Epoch: 35 [12800/28821 (44%)]\tLoss: 1.027990\n",
      "Train Epoch: 35 [14080/28821 (49%)]\tLoss: 1.016402\n",
      "Train Epoch: 35 [15360/28821 (53%)]\tLoss: 0.955807\n",
      "Train Epoch: 35 [16640/28821 (58%)]\tLoss: 1.228724\n",
      "Train Epoch: 35 [17920/28821 (62%)]\tLoss: 1.244320\n",
      "Train Epoch: 35 [19200/28821 (67%)]\tLoss: 1.403335\n",
      "Train Epoch: 35 [20480/28821 (71%)]\tLoss: 1.259852\n",
      "Train Epoch: 35 [21760/28821 (75%)]\tLoss: 1.026034\n",
      "Train Epoch: 35 [23040/28821 (80%)]\tLoss: 1.115330\n",
      "Train Epoch: 35 [24320/28821 (84%)]\tLoss: 1.155941\n",
      "Train Epoch: 35 [25600/28821 (89%)]\tLoss: 1.125095\n",
      "Train Epoch: 35 [26880/28821 (93%)]\tLoss: 1.037377\n",
      "Train Epoch: 35 [28160/28821 (98%)]\tLoss: 1.281937\n",
      "Train Epoch: 36 [0/28821 (0%)]\tLoss: 1.444022\n",
      "Train Epoch: 36 [1280/28821 (4%)]\tLoss: 1.063621\n",
      "Train Epoch: 36 [2560/28821 (9%)]\tLoss: 1.230936\n",
      "Train Epoch: 36 [3840/28821 (13%)]\tLoss: 1.181885\n",
      "Train Epoch: 36 [5120/28821 (18%)]\tLoss: 1.348989\n",
      "Train Epoch: 36 [6400/28821 (22%)]\tLoss: 1.148336\n",
      "Train Epoch: 36 [7680/28821 (27%)]\tLoss: 1.121623\n",
      "Train Epoch: 36 [8960/28821 (31%)]\tLoss: 1.180887\n",
      "Train Epoch: 36 [10240/28821 (35%)]\tLoss: 1.287143\n",
      "Train Epoch: 36 [11520/28821 (40%)]\tLoss: 1.185883\n",
      "Train Epoch: 36 [12800/28821 (44%)]\tLoss: 1.077239\n",
      "Train Epoch: 36 [14080/28821 (49%)]\tLoss: 1.247117\n",
      "Train Epoch: 36 [15360/28821 (53%)]\tLoss: 1.249439\n",
      "Train Epoch: 36 [16640/28821 (58%)]\tLoss: 1.109602\n",
      "Train Epoch: 36 [17920/28821 (62%)]\tLoss: 1.124857\n",
      "Train Epoch: 36 [19200/28821 (67%)]\tLoss: 1.011859\n",
      "Train Epoch: 36 [20480/28821 (71%)]\tLoss: 1.392027\n",
      "Train Epoch: 36 [21760/28821 (75%)]\tLoss: 1.158200\n",
      "Train Epoch: 36 [23040/28821 (80%)]\tLoss: 1.279661\n",
      "Train Epoch: 36 [24320/28821 (84%)]\tLoss: 1.179518\n",
      "Train Epoch: 36 [25600/28821 (89%)]\tLoss: 0.983781\n",
      "Train Epoch: 36 [26880/28821 (93%)]\tLoss: 1.242287\n",
      "Train Epoch: 36 [28160/28821 (98%)]\tLoss: 1.165182\n",
      "Train Epoch: 37 [0/28821 (0%)]\tLoss: 1.099681\n",
      "Train Epoch: 37 [1280/28821 (4%)]\tLoss: 1.282528\n",
      "Train Epoch: 37 [2560/28821 (9%)]\tLoss: 1.345092\n",
      "Train Epoch: 37 [3840/28821 (13%)]\tLoss: 1.303686\n",
      "Train Epoch: 37 [5120/28821 (18%)]\tLoss: 1.249327\n",
      "Train Epoch: 37 [6400/28821 (22%)]\tLoss: 1.001055\n",
      "Train Epoch: 37 [7680/28821 (27%)]\tLoss: 1.185503\n",
      "Train Epoch: 37 [8960/28821 (31%)]\tLoss: 1.246766\n",
      "Train Epoch: 37 [10240/28821 (35%)]\tLoss: 1.364102\n",
      "Train Epoch: 37 [11520/28821 (40%)]\tLoss: 1.221065\n",
      "Train Epoch: 37 [12800/28821 (44%)]\tLoss: 0.902722\n",
      "Train Epoch: 37 [14080/28821 (49%)]\tLoss: 1.179472\n",
      "Train Epoch: 37 [15360/28821 (53%)]\tLoss: 1.039403\n",
      "Train Epoch: 37 [16640/28821 (58%)]\tLoss: 1.250260\n",
      "Train Epoch: 37 [17920/28821 (62%)]\tLoss: 1.266793\n",
      "Train Epoch: 37 [19200/28821 (67%)]\tLoss: 0.946917\n",
      "Train Epoch: 37 [20480/28821 (71%)]\tLoss: 1.081275\n",
      "Train Epoch: 37 [21760/28821 (75%)]\tLoss: 1.427857\n",
      "Train Epoch: 37 [23040/28821 (80%)]\tLoss: 1.098346\n",
      "Train Epoch: 37 [24320/28821 (84%)]\tLoss: 1.278045\n",
      "Train Epoch: 37 [25600/28821 (89%)]\tLoss: 1.261230\n",
      "Train Epoch: 37 [26880/28821 (93%)]\tLoss: 1.115694\n",
      "Train Epoch: 37 [28160/28821 (98%)]\tLoss: 1.124586\n",
      "Train Epoch: 38 [0/28821 (0%)]\tLoss: 1.140451\n",
      "Train Epoch: 38 [1280/28821 (4%)]\tLoss: 1.291942\n",
      "Train Epoch: 38 [2560/28821 (9%)]\tLoss: 1.201183\n",
      "Train Epoch: 38 [3840/28821 (13%)]\tLoss: 0.977610\n",
      "Train Epoch: 38 [5120/28821 (18%)]\tLoss: 1.148825\n",
      "Train Epoch: 38 [6400/28821 (22%)]\tLoss: 1.321958\n",
      "Train Epoch: 38 [7680/28821 (27%)]\tLoss: 1.350136\n",
      "Train Epoch: 38 [8960/28821 (31%)]\tLoss: 1.074962\n",
      "Train Epoch: 38 [10240/28821 (35%)]\tLoss: 1.039183\n",
      "Train Epoch: 38 [11520/28821 (40%)]\tLoss: 1.132695\n",
      "Train Epoch: 38 [12800/28821 (44%)]\tLoss: 1.119362\n",
      "Train Epoch: 38 [14080/28821 (49%)]\tLoss: 1.280088\n",
      "Train Epoch: 38 [15360/28821 (53%)]\tLoss: 1.111879\n",
      "Train Epoch: 38 [16640/28821 (58%)]\tLoss: 1.219457\n",
      "Train Epoch: 38 [17920/28821 (62%)]\tLoss: 1.218636\n",
      "Train Epoch: 38 [19200/28821 (67%)]\tLoss: 1.148305\n",
      "Train Epoch: 38 [20480/28821 (71%)]\tLoss: 1.191689\n",
      "Train Epoch: 38 [21760/28821 (75%)]\tLoss: 1.176206\n",
      "Train Epoch: 38 [23040/28821 (80%)]\tLoss: 1.146648\n",
      "Train Epoch: 38 [24320/28821 (84%)]\tLoss: 1.312864\n",
      "Train Epoch: 38 [25600/28821 (89%)]\tLoss: 1.356743\n",
      "Train Epoch: 38 [26880/28821 (93%)]\tLoss: 1.060228\n",
      "Train Epoch: 38 [28160/28821 (98%)]\tLoss: 1.173010\n",
      "Train Epoch: 39 [0/28821 (0%)]\tLoss: 1.279327\n",
      "Train Epoch: 39 [1280/28821 (4%)]\tLoss: 1.194960\n",
      "Train Epoch: 39 [2560/28821 (9%)]\tLoss: 1.122969\n",
      "Train Epoch: 39 [3840/28821 (13%)]\tLoss: 1.210808\n",
      "Train Epoch: 39 [5120/28821 (18%)]\tLoss: 1.502707\n",
      "Train Epoch: 39 [6400/28821 (22%)]\tLoss: 1.094421\n",
      "Train Epoch: 39 [7680/28821 (27%)]\tLoss: 1.347517\n",
      "Train Epoch: 39 [8960/28821 (31%)]\tLoss: 1.136415\n",
      "Train Epoch: 39 [10240/28821 (35%)]\tLoss: 1.145069\n",
      "Train Epoch: 39 [11520/28821 (40%)]\tLoss: 1.165309\n",
      "Train Epoch: 39 [12800/28821 (44%)]\tLoss: 1.158309\n",
      "Train Epoch: 39 [14080/28821 (49%)]\tLoss: 1.209500\n",
      "Train Epoch: 39 [15360/28821 (53%)]\tLoss: 1.109863\n",
      "Train Epoch: 39 [16640/28821 (58%)]\tLoss: 1.095128\n",
      "Train Epoch: 39 [17920/28821 (62%)]\tLoss: 0.994019\n",
      "Train Epoch: 39 [19200/28821 (67%)]\tLoss: 1.153576\n",
      "Train Epoch: 39 [20480/28821 (71%)]\tLoss: 1.149219\n",
      "Train Epoch: 39 [21760/28821 (75%)]\tLoss: 1.086001\n",
      "Train Epoch: 39 [23040/28821 (80%)]\tLoss: 1.090686\n",
      "Train Epoch: 39 [24320/28821 (84%)]\tLoss: 1.254816\n",
      "Train Epoch: 39 [25600/28821 (89%)]\tLoss: 1.040385\n",
      "Train Epoch: 39 [26880/28821 (93%)]\tLoss: 1.216788\n",
      "Train Epoch: 39 [28160/28821 (98%)]\tLoss: 1.142029\n",
      "Train Epoch: 40 [0/28821 (0%)]\tLoss: 1.312806\n",
      "Train Epoch: 40 [1280/28821 (4%)]\tLoss: 1.060866\n",
      "Train Epoch: 40 [2560/28821 (9%)]\tLoss: 1.150626\n",
      "Train Epoch: 40 [3840/28821 (13%)]\tLoss: 1.349425\n",
      "Train Epoch: 40 [5120/28821 (18%)]\tLoss: 1.034043\n",
      "Train Epoch: 40 [6400/28821 (22%)]\tLoss: 1.201842\n",
      "Train Epoch: 40 [7680/28821 (27%)]\tLoss: 1.050385\n",
      "Train Epoch: 40 [8960/28821 (31%)]\tLoss: 1.207507\n",
      "Train Epoch: 40 [10240/28821 (35%)]\tLoss: 1.260345\n",
      "Train Epoch: 40 [11520/28821 (40%)]\tLoss: 1.147752\n",
      "Train Epoch: 40 [12800/28821 (44%)]\tLoss: 1.247308\n",
      "Train Epoch: 40 [14080/28821 (49%)]\tLoss: 1.134022\n",
      "Train Epoch: 40 [15360/28821 (53%)]\tLoss: 1.327235\n",
      "Train Epoch: 40 [16640/28821 (58%)]\tLoss: 1.065531\n",
      "Train Epoch: 40 [17920/28821 (62%)]\tLoss: 1.079458\n",
      "Train Epoch: 40 [19200/28821 (67%)]\tLoss: 1.270973\n",
      "Train Epoch: 40 [20480/28821 (71%)]\tLoss: 0.925461\n",
      "Train Epoch: 40 [21760/28821 (75%)]\tLoss: 1.365069\n",
      "Train Epoch: 40 [23040/28821 (80%)]\tLoss: 1.184352\n",
      "Train Epoch: 40 [24320/28821 (84%)]\tLoss: 0.973051\n",
      "Train Epoch: 40 [25600/28821 (89%)]\tLoss: 1.030977\n",
      "Train Epoch: 40 [26880/28821 (93%)]\tLoss: 1.163831\n",
      "Train Epoch: 40 [28160/28821 (98%)]\tLoss: 1.187112\n",
      "Train Epoch: 41 [0/28821 (0%)]\tLoss: 1.162637\n",
      "Train Epoch: 41 [1280/28821 (4%)]\tLoss: 1.104629\n",
      "Train Epoch: 41 [2560/28821 (9%)]\tLoss: 1.177213\n",
      "Train Epoch: 41 [3840/28821 (13%)]\tLoss: 1.103608\n",
      "Train Epoch: 41 [5120/28821 (18%)]\tLoss: 1.118938\n",
      "Train Epoch: 41 [6400/28821 (22%)]\tLoss: 1.109090\n",
      "Train Epoch: 41 [7680/28821 (27%)]\tLoss: 1.027354\n",
      "Train Epoch: 41 [8960/28821 (31%)]\tLoss: 1.086379\n",
      "Train Epoch: 41 [10240/28821 (35%)]\tLoss: 1.147050\n",
      "Train Epoch: 41 [11520/28821 (40%)]\tLoss: 1.127524\n",
      "Train Epoch: 41 [12800/28821 (44%)]\tLoss: 1.221876\n",
      "Train Epoch: 41 [14080/28821 (49%)]\tLoss: 0.990482\n",
      "Train Epoch: 41 [15360/28821 (53%)]\tLoss: 1.208506\n",
      "Train Epoch: 41 [16640/28821 (58%)]\tLoss: 1.132029\n",
      "Train Epoch: 41 [17920/28821 (62%)]\tLoss: 1.143513\n",
      "Train Epoch: 41 [19200/28821 (67%)]\tLoss: 1.137956\n",
      "Train Epoch: 41 [20480/28821 (71%)]\tLoss: 1.041753\n",
      "Train Epoch: 41 [21760/28821 (75%)]\tLoss: 1.213646\n",
      "Train Epoch: 41 [23040/28821 (80%)]\tLoss: 0.945606\n",
      "Train Epoch: 41 [24320/28821 (84%)]\tLoss: 1.174953\n",
      "Train Epoch: 41 [25600/28821 (89%)]\tLoss: 1.097693\n",
      "Train Epoch: 41 [26880/28821 (93%)]\tLoss: 1.052747\n",
      "Train Epoch: 41 [28160/28821 (98%)]\tLoss: 1.139123\n",
      "Train Epoch: 42 [0/28821 (0%)]\tLoss: 1.221591\n",
      "Train Epoch: 42 [1280/28821 (4%)]\tLoss: 1.350113\n",
      "Train Epoch: 42 [2560/28821 (9%)]\tLoss: 0.966379\n",
      "Train Epoch: 42 [3840/28821 (13%)]\tLoss: 1.198912\n",
      "Train Epoch: 42 [5120/28821 (18%)]\tLoss: 1.203703\n",
      "Train Epoch: 42 [6400/28821 (22%)]\tLoss: 1.249452\n",
      "Train Epoch: 42 [7680/28821 (27%)]\tLoss: 1.023277\n",
      "Train Epoch: 42 [8960/28821 (31%)]\tLoss: 1.176281\n",
      "Train Epoch: 42 [10240/28821 (35%)]\tLoss: 1.277989\n",
      "Train Epoch: 42 [11520/28821 (40%)]\tLoss: 1.312331\n",
      "Train Epoch: 42 [12800/28821 (44%)]\tLoss: 1.212953\n",
      "Train Epoch: 42 [14080/28821 (49%)]\tLoss: 1.261612\n",
      "Train Epoch: 42 [15360/28821 (53%)]\tLoss: 1.445482\n",
      "Train Epoch: 42 [16640/28821 (58%)]\tLoss: 1.035848\n",
      "Train Epoch: 42 [17920/28821 (62%)]\tLoss: 1.082421\n",
      "Train Epoch: 42 [19200/28821 (67%)]\tLoss: 1.328322\n",
      "Train Epoch: 42 [20480/28821 (71%)]\tLoss: 1.240567\n",
      "Train Epoch: 42 [21760/28821 (75%)]\tLoss: 1.149104\n",
      "Train Epoch: 42 [23040/28821 (80%)]\tLoss: 1.325506\n",
      "Train Epoch: 42 [24320/28821 (84%)]\tLoss: 1.158913\n",
      "Train Epoch: 42 [25600/28821 (89%)]\tLoss: 1.163867\n",
      "Train Epoch: 42 [26880/28821 (93%)]\tLoss: 1.170609\n",
      "Train Epoch: 42 [28160/28821 (98%)]\tLoss: 1.073718\n",
      "Train Epoch: 43 [0/28821 (0%)]\tLoss: 1.264091\n",
      "Train Epoch: 43 [1280/28821 (4%)]\tLoss: 0.939269\n",
      "Train Epoch: 43 [2560/28821 (9%)]\tLoss: 1.188795\n",
      "Train Epoch: 43 [3840/28821 (13%)]\tLoss: 1.273801\n",
      "Train Epoch: 43 [5120/28821 (18%)]\tLoss: 1.085557\n",
      "Train Epoch: 43 [6400/28821 (22%)]\tLoss: 1.101933\n",
      "Train Epoch: 43 [7680/28821 (27%)]\tLoss: 1.377212\n",
      "Train Epoch: 43 [8960/28821 (31%)]\tLoss: 1.244807\n",
      "Train Epoch: 43 [10240/28821 (35%)]\tLoss: 1.100584\n",
      "Train Epoch: 43 [11520/28821 (40%)]\tLoss: 1.156186\n",
      "Train Epoch: 43 [12800/28821 (44%)]\tLoss: 1.239426\n",
      "Train Epoch: 43 [14080/28821 (49%)]\tLoss: 1.050333\n",
      "Train Epoch: 43 [15360/28821 (53%)]\tLoss: 1.114745\n",
      "Train Epoch: 43 [16640/28821 (58%)]\tLoss: 1.206029\n",
      "Train Epoch: 43 [17920/28821 (62%)]\tLoss: 1.224167\n",
      "Train Epoch: 43 [19200/28821 (67%)]\tLoss: 1.183523\n",
      "Train Epoch: 43 [20480/28821 (71%)]\tLoss: 1.266337\n",
      "Train Epoch: 43 [21760/28821 (75%)]\tLoss: 1.117357\n",
      "Train Epoch: 43 [23040/28821 (80%)]\tLoss: 1.151617\n",
      "Train Epoch: 43 [24320/28821 (84%)]\tLoss: 1.418767\n",
      "Train Epoch: 43 [25600/28821 (89%)]\tLoss: 1.254610\n",
      "Train Epoch: 43 [26880/28821 (93%)]\tLoss: 1.213008\n",
      "Train Epoch: 43 [28160/28821 (98%)]\tLoss: 1.185127\n",
      "Train Epoch: 44 [0/28821 (0%)]\tLoss: 1.245560\n",
      "Train Epoch: 44 [1280/28821 (4%)]\tLoss: 1.122810\n",
      "Train Epoch: 44 [2560/28821 (9%)]\tLoss: 1.294209\n",
      "Train Epoch: 44 [3840/28821 (13%)]\tLoss: 1.072279\n",
      "Train Epoch: 44 [5120/28821 (18%)]\tLoss: 1.082698\n",
      "Train Epoch: 44 [6400/28821 (22%)]\tLoss: 1.131214\n",
      "Train Epoch: 44 [7680/28821 (27%)]\tLoss: 1.170753\n",
      "Train Epoch: 44 [8960/28821 (31%)]\tLoss: 1.159400\n",
      "Train Epoch: 44 [10240/28821 (35%)]\tLoss: 0.964527\n",
      "Train Epoch: 44 [11520/28821 (40%)]\tLoss: 1.231707\n",
      "Train Epoch: 44 [12800/28821 (44%)]\tLoss: 1.244591\n",
      "Train Epoch: 44 [14080/28821 (49%)]\tLoss: 1.035189\n",
      "Train Epoch: 44 [15360/28821 (53%)]\tLoss: 1.095422\n",
      "Train Epoch: 44 [16640/28821 (58%)]\tLoss: 1.028111\n",
      "Train Epoch: 44 [17920/28821 (62%)]\tLoss: 1.204942\n",
      "Train Epoch: 44 [19200/28821 (67%)]\tLoss: 1.208661\n",
      "Train Epoch: 44 [20480/28821 (71%)]\tLoss: 1.074498\n",
      "Train Epoch: 44 [21760/28821 (75%)]\tLoss: 1.386994\n",
      "Train Epoch: 44 [23040/28821 (80%)]\tLoss: 1.081830\n",
      "Train Epoch: 44 [24320/28821 (84%)]\tLoss: 1.125099\n",
      "Train Epoch: 44 [25600/28821 (89%)]\tLoss: 1.220153\n",
      "Train Epoch: 44 [26880/28821 (93%)]\tLoss: 1.226695\n",
      "Train Epoch: 44 [28160/28821 (98%)]\tLoss: 1.355895\n",
      "Train Epoch: 45 [0/28821 (0%)]\tLoss: 1.153630\n",
      "Train Epoch: 45 [1280/28821 (4%)]\tLoss: 1.461409\n",
      "Train Epoch: 45 [2560/28821 (9%)]\tLoss: 1.090778\n",
      "Train Epoch: 45 [3840/28821 (13%)]\tLoss: 0.954646\n",
      "Train Epoch: 45 [5120/28821 (18%)]\tLoss: 1.113033\n",
      "Train Epoch: 45 [6400/28821 (22%)]\tLoss: 1.233435\n",
      "Train Epoch: 45 [7680/28821 (27%)]\tLoss: 1.124935\n",
      "Train Epoch: 45 [8960/28821 (31%)]\tLoss: 1.237869\n",
      "Train Epoch: 45 [10240/28821 (35%)]\tLoss: 1.116448\n",
      "Train Epoch: 45 [11520/28821 (40%)]\tLoss: 1.195074\n",
      "Train Epoch: 45 [12800/28821 (44%)]\tLoss: 1.088176\n",
      "Train Epoch: 45 [14080/28821 (49%)]\tLoss: 1.232498\n",
      "Train Epoch: 45 [15360/28821 (53%)]\tLoss: 1.145053\n",
      "Train Epoch: 45 [16640/28821 (58%)]\tLoss: 1.245993\n",
      "Train Epoch: 45 [17920/28821 (62%)]\tLoss: 1.044821\n",
      "Train Epoch: 45 [19200/28821 (67%)]\tLoss: 1.176765\n",
      "Train Epoch: 45 [20480/28821 (71%)]\tLoss: 1.087701\n",
      "Train Epoch: 45 [21760/28821 (75%)]\tLoss: 1.084016\n",
      "Train Epoch: 45 [23040/28821 (80%)]\tLoss: 1.368761\n",
      "Train Epoch: 45 [24320/28821 (84%)]\tLoss: 1.244856\n",
      "Train Epoch: 45 [25600/28821 (89%)]\tLoss: 1.048173\n",
      "Train Epoch: 45 [26880/28821 (93%)]\tLoss: 0.981568\n",
      "Train Epoch: 45 [28160/28821 (98%)]\tLoss: 1.099431\n",
      "Train Epoch: 46 [0/28821 (0%)]\tLoss: 1.087507\n",
      "Train Epoch: 46 [1280/28821 (4%)]\tLoss: 1.121053\n",
      "Train Epoch: 46 [2560/28821 (9%)]\tLoss: 1.159244\n",
      "Train Epoch: 46 [3840/28821 (13%)]\tLoss: 1.267789\n",
      "Train Epoch: 46 [5120/28821 (18%)]\tLoss: 1.166215\n",
      "Train Epoch: 46 [6400/28821 (22%)]\tLoss: 1.269255\n",
      "Train Epoch: 46 [7680/28821 (27%)]\tLoss: 1.135520\n",
      "Train Epoch: 46 [8960/28821 (31%)]\tLoss: 1.299328\n",
      "Train Epoch: 46 [10240/28821 (35%)]\tLoss: 1.217763\n",
      "Train Epoch: 46 [11520/28821 (40%)]\tLoss: 1.275651\n",
      "Train Epoch: 46 [12800/28821 (44%)]\tLoss: 1.323013\n",
      "Train Epoch: 46 [14080/28821 (49%)]\tLoss: 1.012732\n",
      "Train Epoch: 46 [15360/28821 (53%)]\tLoss: 1.196114\n",
      "Train Epoch: 46 [16640/28821 (58%)]\tLoss: 1.015664\n",
      "Train Epoch: 46 [17920/28821 (62%)]\tLoss: 1.097559\n",
      "Train Epoch: 46 [19200/28821 (67%)]\tLoss: 1.086570\n",
      "Train Epoch: 46 [20480/28821 (71%)]\tLoss: 1.177875\n",
      "Train Epoch: 46 [21760/28821 (75%)]\tLoss: 1.127385\n",
      "Train Epoch: 46 [23040/28821 (80%)]\tLoss: 0.937936\n",
      "Train Epoch: 46 [24320/28821 (84%)]\tLoss: 1.190879\n",
      "Train Epoch: 46 [25600/28821 (89%)]\tLoss: 1.402704\n",
      "Train Epoch: 46 [26880/28821 (93%)]\tLoss: 0.954710\n",
      "Train Epoch: 46 [28160/28821 (98%)]\tLoss: 1.120161\n",
      "Train Epoch: 47 [0/28821 (0%)]\tLoss: 1.114155\n",
      "Train Epoch: 47 [1280/28821 (4%)]\tLoss: 1.101933\n",
      "Train Epoch: 47 [2560/28821 (9%)]\tLoss: 1.269990\n",
      "Train Epoch: 47 [3840/28821 (13%)]\tLoss: 1.414078\n",
      "Train Epoch: 47 [5120/28821 (18%)]\tLoss: 1.326934\n",
      "Train Epoch: 47 [6400/28821 (22%)]\tLoss: 1.026320\n",
      "Train Epoch: 47 [7680/28821 (27%)]\tLoss: 1.207007\n",
      "Train Epoch: 47 [8960/28821 (31%)]\tLoss: 1.313291\n",
      "Train Epoch: 47 [10240/28821 (35%)]\tLoss: 1.149540\n",
      "Train Epoch: 47 [11520/28821 (40%)]\tLoss: 1.239114\n",
      "Train Epoch: 47 [12800/28821 (44%)]\tLoss: 1.207958\n",
      "Train Epoch: 47 [14080/28821 (49%)]\tLoss: 1.251072\n",
      "Train Epoch: 47 [15360/28821 (53%)]\tLoss: 0.926968\n",
      "Train Epoch: 47 [16640/28821 (58%)]\tLoss: 1.304731\n",
      "Train Epoch: 47 [17920/28821 (62%)]\tLoss: 1.112262\n",
      "Train Epoch: 47 [19200/28821 (67%)]\tLoss: 1.305883\n",
      "Train Epoch: 47 [20480/28821 (71%)]\tLoss: 1.224411\n",
      "Train Epoch: 47 [21760/28821 (75%)]\tLoss: 1.202596\n",
      "Train Epoch: 47 [23040/28821 (80%)]\tLoss: 1.114503\n",
      "Train Epoch: 47 [24320/28821 (84%)]\tLoss: 1.175516\n",
      "Train Epoch: 47 [25600/28821 (89%)]\tLoss: 1.246949\n",
      "Train Epoch: 47 [26880/28821 (93%)]\tLoss: 0.966041\n",
      "Train Epoch: 47 [28160/28821 (98%)]\tLoss: 1.352863\n",
      "Train Epoch: 48 [0/28821 (0%)]\tLoss: 1.080268\n",
      "Train Epoch: 48 [1280/28821 (4%)]\tLoss: 1.313951\n",
      "Train Epoch: 48 [2560/28821 (9%)]\tLoss: 1.111475\n",
      "Train Epoch: 48 [3840/28821 (13%)]\tLoss: 1.192308\n",
      "Train Epoch: 48 [5120/28821 (18%)]\tLoss: 1.083795\n",
      "Train Epoch: 48 [6400/28821 (22%)]\tLoss: 1.052210\n",
      "Train Epoch: 48 [7680/28821 (27%)]\tLoss: 0.926339\n",
      "Train Epoch: 48 [8960/28821 (31%)]\tLoss: 1.292510\n",
      "Train Epoch: 48 [10240/28821 (35%)]\tLoss: 1.341574\n",
      "Train Epoch: 48 [11520/28821 (40%)]\tLoss: 1.112604\n",
      "Train Epoch: 48 [12800/28821 (44%)]\tLoss: 1.107606\n",
      "Train Epoch: 48 [14080/28821 (49%)]\tLoss: 0.966691\n",
      "Train Epoch: 48 [15360/28821 (53%)]\tLoss: 1.023444\n",
      "Train Epoch: 48 [16640/28821 (58%)]\tLoss: 1.262412\n",
      "Train Epoch: 48 [17920/28821 (62%)]\tLoss: 1.252356\n",
      "Train Epoch: 48 [19200/28821 (67%)]\tLoss: 1.068484\n",
      "Train Epoch: 48 [20480/28821 (71%)]\tLoss: 0.988670\n",
      "Train Epoch: 48 [21760/28821 (75%)]\tLoss: 1.207331\n",
      "Train Epoch: 48 [23040/28821 (80%)]\tLoss: 1.262720\n",
      "Train Epoch: 48 [24320/28821 (84%)]\tLoss: 1.048801\n",
      "Train Epoch: 48 [25600/28821 (89%)]\tLoss: 1.286863\n",
      "Train Epoch: 48 [26880/28821 (93%)]\tLoss: 1.162951\n",
      "Train Epoch: 48 [28160/28821 (98%)]\tLoss: 1.075503\n",
      "Train Epoch: 49 [0/28821 (0%)]\tLoss: 0.985220\n",
      "Train Epoch: 49 [1280/28821 (4%)]\tLoss: 1.158466\n",
      "Train Epoch: 49 [2560/28821 (9%)]\tLoss: 1.271698\n",
      "Train Epoch: 49 [3840/28821 (13%)]\tLoss: 1.210630\n",
      "Train Epoch: 49 [5120/28821 (18%)]\tLoss: 1.225654\n",
      "Train Epoch: 49 [6400/28821 (22%)]\tLoss: 1.297323\n",
      "Train Epoch: 49 [7680/28821 (27%)]\tLoss: 1.079659\n",
      "Train Epoch: 49 [8960/28821 (31%)]\tLoss: 1.027522\n",
      "Train Epoch: 49 [10240/28821 (35%)]\tLoss: 1.057265\n",
      "Train Epoch: 49 [11520/28821 (40%)]\tLoss: 1.268741\n",
      "Train Epoch: 49 [12800/28821 (44%)]\tLoss: 1.158905\n",
      "Train Epoch: 49 [14080/28821 (49%)]\tLoss: 1.343402\n",
      "Train Epoch: 49 [15360/28821 (53%)]\tLoss: 1.256005\n",
      "Train Epoch: 49 [16640/28821 (58%)]\tLoss: 1.136820\n",
      "Train Epoch: 49 [17920/28821 (62%)]\tLoss: 1.351352\n",
      "Train Epoch: 49 [19200/28821 (67%)]\tLoss: 1.111477\n",
      "Train Epoch: 49 [20480/28821 (71%)]\tLoss: 1.280154\n",
      "Train Epoch: 49 [21760/28821 (75%)]\tLoss: 0.903704\n",
      "Train Epoch: 49 [23040/28821 (80%)]\tLoss: 1.215569\n",
      "Train Epoch: 49 [24320/28821 (84%)]\tLoss: 1.073657\n",
      "Train Epoch: 49 [25600/28821 (89%)]\tLoss: 0.988747\n",
      "Train Epoch: 49 [26880/28821 (93%)]\tLoss: 1.364280\n",
      "Train Epoch: 49 [28160/28821 (98%)]\tLoss: 1.062981\n",
      "Train Epoch: 50 [0/28821 (0%)]\tLoss: 1.040511\n",
      "Train Epoch: 50 [1280/28821 (4%)]\tLoss: 1.190681\n",
      "Train Epoch: 50 [2560/28821 (9%)]\tLoss: 0.989470\n",
      "Train Epoch: 50 [3840/28821 (13%)]\tLoss: 1.216696\n",
      "Train Epoch: 50 [5120/28821 (18%)]\tLoss: 1.145410\n",
      "Train Epoch: 50 [6400/28821 (22%)]\tLoss: 1.136130\n",
      "Train Epoch: 50 [7680/28821 (27%)]\tLoss: 1.195928\n",
      "Train Epoch: 50 [8960/28821 (31%)]\tLoss: 1.168257\n",
      "Train Epoch: 50 [10240/28821 (35%)]\tLoss: 1.091536\n",
      "Train Epoch: 50 [11520/28821 (40%)]\tLoss: 1.012006\n",
      "Train Epoch: 50 [12800/28821 (44%)]\tLoss: 1.295481\n",
      "Train Epoch: 50 [14080/28821 (49%)]\tLoss: 1.244802\n",
      "Train Epoch: 50 [15360/28821 (53%)]\tLoss: 1.401958\n",
      "Train Epoch: 50 [16640/28821 (58%)]\tLoss: 0.940616\n",
      "Train Epoch: 50 [17920/28821 (62%)]\tLoss: 1.159256\n",
      "Train Epoch: 50 [19200/28821 (67%)]\tLoss: 1.217853\n",
      "Train Epoch: 50 [20480/28821 (71%)]\tLoss: 1.229806\n",
      "Train Epoch: 50 [21760/28821 (75%)]\tLoss: 1.383465\n",
      "Train Epoch: 50 [23040/28821 (80%)]\tLoss: 1.162534\n",
      "Train Epoch: 50 [24320/28821 (84%)]\tLoss: 1.318101\n",
      "Train Epoch: 50 [25600/28821 (89%)]\tLoss: 0.983819\n",
      "Train Epoch: 50 [26880/28821 (93%)]\tLoss: 1.050056\n",
      "Train Epoch: 50 [28160/28821 (98%)]\tLoss: 1.089992\n"
     ]
    }
   ],
   "source": [
    "n_epoch = 50\n",
    "log_interval=20\n",
    "\n",
    "losses = []\n",
    "\n",
    "# The transform needs to live on the same device as the model and the data.\n",
    "for epoch in range(1, n_epoch + 1):\n",
    "  train(model, epoch, log_interval)\n",
    "  # test(model, epoch)\n",
    "  scheduler.step()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 269,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\fuchsga\\AppData\\Local\\Temp\\ipykernel_29756\\2981391630.py:38: UserWarning: Implicit dimension choice for log_softmax has been deprecated. Change the call to include dim=X as an argument.\n",
      "  return nn.functional.log_softmax(x)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor(58.4772, device='cuda:0')\n"
     ]
    }
   ],
   "source": [
    "tot = 0\n",
    "corr = 0\n",
    "model.eval()\n",
    "for _, (data, target) in enumerate(valid_loader):\n",
    "    data = data.to(device)\n",
    "    target = target.to(device)\n",
    "    pred = model(data)\n",
    "    pred = pred.argmax(dim=-1)\n",
    "    correct = pred == target\n",
    "    corr += sum(correct)\n",
    "    tot += len(data)\n",
    "\n",
    "print(str(corr/tot*100))\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 268,
   "metadata": {},
   "outputs": [],
   "source": [
    "save_path = \"C:\\\\Users\\\\fuchsga\\\\Desktop\\\\WebApps\\\\FacialAnalysisWebApp\\\\model2.pt\"\n",
    "torch.save(model.state_dict(), save_path)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 274,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "torch.Size([1, 48, 48])"
      ]
     },
     "execution_count": 274,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "data[0].shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "base",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.13"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
